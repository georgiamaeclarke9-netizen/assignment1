{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a504eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(\"\"\"pandas>=1.5.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "matplotlib>=3.5.0\n",
    "seaborn>=0.11.0\n",
    "xgboost>=1.7.6\n",
    "yfinance>=0.2.0\n",
    "sklearn>=1.6.1            \n",
    "statsmodels>=0.13.0\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mltester import run_mltester # remove?\n",
    "import sys\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Type\n",
    "\n",
    "# Python 3.13.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8807081",
   "metadata": {},
   "source": [
    "## NOTE; HOW TO RUN CODE/NOTEBOOK FOR ASSESSOR:\n",
    "\n",
    "**Use of Artificial Intelligence Acknowledgement: Some Files within this assignment have been created with the assistance of Microsoft Copilot - indicated by a top comment**\n",
    "\n",
    "### Option 1: Estimated Completion Time: \n",
    "If you wish to run the notebook, you can do so via the \"Run All\" button on the very top.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "\n",
    "### Option 2: Estimated Completion Time: \n",
    "If you wish to run only the required code; run the first two Python cells in this notebook and search (Ctrl+F) in this notebook for **%run DataPreparation/KMeans_Data_Prep_robustScaler.py** and run the cell, then search for **%run student_ext.py** and run this cell. \n",
    "Alternatively, you can run them via terminal commands (ensure you are running from project root):\n",
    "\n",
    "**via macOS/Linux:**\n",
    "#### Option A: Run with the default py \n",
    "python DataPreparation/KMeans_Data_Prep_robustScaler.py\n",
    "python student_ext.py\n",
    "\n",
    "#### Option B: If your default is Python 2, use python3 explicitly\n",
    "python3 DataPreparation/KMeans_Data_Prep_robustScaler.py\n",
    "python3 student_ext.py\n",
    "\n",
    "\n",
    "\n",
    "**via PowerShell/CommandPrompt**:\n",
    "#### PowerShell\n",
    "python .\\DataPreparation\\KMeans_Data_Prep_robustScaler.py\n",
    "python .\\student_ext.py\n",
    "\n",
    "#### If needed, explicitly use py launcher to select Python 3\n",
    "py -3 .\\DataPreparation\\KMeans_Data_Prep_robustScaler.py\n",
    "py -3 .\\student_ext.py\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "To save on time, I have included my part 1 results, my original clustering model, experimentations of my clustering model, and baseline comparison as .csv files and I have imported their findings into this notebook. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f56e1",
   "metadata": {},
   "source": [
    "# Part 2: Multivariate Regime Detection and Predictive Modelling\n",
    "\n",
    "Noted suggested improvements from previous assingment results:\n",
    "\"the treatment of leakage and refit strategy appears technically sound, but the written justification is quite thin. There is limited explicit discussion of exactly how leakage is/should be avoided or how the walk-forward refitting behaves in practice. Similarly, there is not much evidence of feature engineering beyond what is in the baseline: it would be good to see experiments or ablations with alternative feature sets, and some evidence/arguments about which features are most important or why\"\n",
    "\n",
    "\n",
    "Within this project, I will be performing the following:\n",
    "\n",
    "#### 1. Data Preparation:\n",
    "\"Using the same set of tickers as in Part 1 (XLK, XLP, XLV, XLF, XLE, XLI) to construct a multivariate timeseries from daily or weekly prices or other derived features (such as returns and volatility).\"\n",
    "\n",
    "**For this assignment, I have decided upon using weekly adj_close prices. I used weekly data for clustering as:\n",
    "- weekly data has less noise compared to daily data, \n",
    "- rolling stats can be more stable on weekly data, compared to more volatile daily data,\n",
    "- less computational cost, leading to faster clustering\n",
    "\n",
    "As mentioned in McGreevy et al's \"Detecting multivariate market regimes via clustering algorithms\" (2024),[1]  data is sized to aproximate weekly windows: \"We assume that we want to trade over a specific window and take h1 = 35 equivalent to one market week in market hours\"\n",
    "\n",
    "Within this assignment, I use adj_close as it accounts for dividends, corporate actions and splits - which leads to better analysis for long-term investing.**\n",
    "\n",
    "#### 2. Regime Detection:\n",
    "\n",
    "\"Apply a clustering algorithm of your choice (e.g. k-means, Gaussian Mixture Models, hierarchical clustering) To identify latent regimes (ie clusters) from this multivariate time series.\"\n",
    "\n",
    "For this assignment, I will be attempting the clustering algorithm; K-Means.\n",
    "\n",
    "#### 3. Integration with Prediction\n",
    "\"Use the detected regime labels as additional features in your predictive modelling task from Part 1 (the Student class with fit/predict).\n",
    "• Evaluate whether regime-informed prediction improves over your Part 1 Student class as the baseline\"\n",
    "\n",
    "For this assignment, I will attempt to integrate my chosen clustering algorithm with my original part 1 baseline student.py and mltester. I will also run experiments to see if certain fields/methods, e.t.c. changed within my *Regime Detection* section can help improve the overall Directional Accuracy and decrease MAE and RMSE.\n",
    "\n",
    "#### 4. Baseline Comparison\n",
    "\n",
    "\"In addition to your own Part 1 baseline, you must compare your results against at least one published baseline method\" I will be using: \"McGreevy, J., Muguruza, A., Issa, Z., Salvi, C., Chan, J., & Zuric, Z. (2024). Detecting Multivariate Market Regimes Via Clustering Algorithms. SSRN [http://dx.doi.org/10.2139/ssrn.4758243]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390fb43",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "The first clustering algorithm I'm attempting is K-Means Clustering. K-means has been proven as a successful clustering algorithm in previous academic papers, such as \"Horvath, Issa & Muguruza (2021); Clustering Market Regimes Using the Wasserstein Distance\"[2]; found that using a variant of K-means (WK-means) got high accuracy in detecting market regimes in univariate time series.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720b599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "K = 2\n",
      "Silhouette: 0.334 | CH: 140.4\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0072, vol=0.0113 -> BULL\n",
      "Cluster 1: ret=-0.0193, vol=0.0140 -> BEAR\n",
      "Saved: detected_regimes_k2.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 3\n",
      "Silhouette: 0.303 | CH: 141.0\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0204, vol=0.0137 -> BULL\n",
      "Cluster 1: ret=0.0035, vol=0.0109 -> NEUTRAL\n",
      "Cluster 2: ret=-0.0300, vol=0.0141 -> BEAR\n",
      "Saved: detected_regimes_k3.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 4\n",
      "Silhouette: 0.149 | CH: 120.1\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=-0.0065, vol=0.0111 -> NEUTRAL\n",
      "Cluster 1: ret=0.0192, vol=0.0142 -> NEUTRAL\n",
      "Cluster 2: ret=-0.0353, vol=0.0157 -> BEAR\n",
      "Cluster 3: ret=0.0116, vol=0.0106 -> BULL\n",
      "Saved: detected_regimes_k4.csv\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run DataPreparation/KMeans_Data_Prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab6f68",
   "metadata": {},
   "source": [
    "Chicken and egg problem?\n",
    "\n",
    "\n",
    "Within Appendix/DataPreparation/KMeans_Data_Prep.py; I loaded daily prices from prices.csv (specifically adj_close), normalised tickers and converted long format into a wide panel of the six sector ETFs. From this weekly panel, I created causal features including weekly log returns, 12-week rolling volatility and 6-week momentum for each ticker, along with market level aggregates (i.e. cross sectional volaitility) for capturing overall market conditions. I drop rows with incomplete features and then split the dataset into train and test sets based on a fixed date (I chose 4th Jan 2019 as it is a Friday, it provides 9 years of training data (2010-2018) and 5 years of testing data (2019-2023)) as to avoid look-ahead bias. \n",
    "\n",
    "ISLP[3] suggests \"We strongly recommend always running K-means clustering with a large value of n_init, such as 20 or 50, since otherwise an undesirable local optimum may be obtained\", hence why my n_init=50. Later in this assignment, I will trying n_init values of 20, 30, 40 and 50.\n",
    "\n",
    "As a practical consideration, for standardising inputs, I use StandardScaler to minimise the within-cluster Euclidean distances. As mentioned on page 532 of ISLP [3], standardisation is preferred on K-means, so later in this assignment, I will be trying different scalers from the sklearn.preprocessing package to see which returns the best directional accuracy and lowest MAE and RMSE.\n",
    "\n",
    "To understand the best K value for my k-means clustering on the prices.csv data, I've opted to use a mixture of silhouette score and Calinski-Harabasz scores. To combat data leakage, I've included the creation of the regime_lag1 so that predictiosn only use information available at prediction time. \n",
    "\n",
    "\n",
    "Interpreting the Data;\n",
    "The terms; \"Bull\", \"Neutral\" and \"Bear\" are used to refer to stock market conditions - used to describe how the market is doing in general. According to Investopedia;( https://www.investopedia.com/insights/digging-deeper-bull-and-bear-markets/ ), \"A bull market is a market that is on the rise and where the economy is sound. A bear market exists in an economy that is receding, where most stocks are declining in value.\" Neutral market indicates a market that is stable, which is neither rising, nor falling.\n",
    "\n",
    "Within my data preparation file, I have a heuristic rule for the different K values.\n",
    "\n",
    "For K=2, I use binary classification on returns, where if average return is posivtive it is a \"Bull\", whereas negative it is \"Bear\".\n",
    "\n",
    "For K=3 and K=4, I use ternary classification on return and volatility, to create a 2x2 decision matrix:\n",
    "\n",
    "| Return VS Median |  Volatility VS Median | Label    |\n",
    "|----------        |---------             -|----------|\n",
    "| Above Median     | Below Median          | Bull     |\n",
    "| Above Median     | Above Median          | Neutral  |\n",
    "| Below Median     | Below Median          | Neutral  |\n",
    "| Below Median     | Above Median          | Bear     |\n",
    "\n",
    "I use siholuette and Calinski-Harabasz to understand what would be the best K value. \n",
    "\n",
    "Use of Silhouette Score:\n",
    "The silhouette score ranges from -1 to 1; \"with the score being bounded between -1 and 1. [A positive value] indicates that the point is appropriately clustered, whereas values near 0 denote ambiguity, and negative values hint at a possible misclassification.\" according to NumberAnalytics ( https://www.numberanalytics.com/blog/silhouette-score-clustering-evaluation ).\n",
    "\n",
    "Use of Calinski-Harabasz (CH):\n",
    "The Calinski-Harabasz index (also known as variance ratio criterion) measures \"how similar an object is to it's own cluster (cohesion) compared to other clusters (seperation)\" according to GeeksForGeeks ( https://www.geeksforgeeks.org/machine-learning/calinski-harabasz-index-cluster-validity-indices-set-3/ ). A higher value for CH means that clusters are dense are well seperated. \n",
    "\n",
    "Results:\n",
    "For K=2:\n",
    "- Silhouette Score is 0.334, CH Index is 140.4\n",
    "- Cluster 0 is bull\n",
    "- Cluster 1 is bear\n",
    "\n",
    "The silhouette score is the highest out of the different K values, indicating the clearest regime distinction. This is the most optimal k value as it's the simplest model for capturing the main market states.\n",
    "\n",
    "For K=3:\n",
    "-  Silhouette Score is 0.303, CH index is 141.0\n",
    "- Cluster 0 is bull\n",
    "- Cluster 1 is neutral\n",
    "- Cluster 2 is bear\n",
    "\n",
    "The silhouette score is lower than it is for k=2, but CH index is slightly higher. Silhouette score is more important for intrepability however, as Silhouette has a bounded range (-1 to 1) compared to CH, which has no bounded range. This k value captures netural periods (slightly positive returns, low volatility).\n",
    "\n",
    "For K=4\n",
    "- Silhouette Score is 0.149, CH index is 120.1\n",
    "\n",
    "The Silhouette score drops dramatically to 0.149 and CH index also has the lowest value out of all the k values; indicating the worst seperation.\n",
    "\n",
    "For this assignment, I will be using k=2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2883c",
   "metadata": {},
   "source": [
    "### Integrating with Prediction:\n",
    "\n",
    "Below is my baseline student from Coursework Part 1, saved as \"student.py\" which uses ElasticNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bffa7b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   XLE: DirAcc=0.5072  MAE=0.011832  RMSE=0.017435\n",
      "   XLF: DirAcc=0.5111  MAE=0.009444  RMSE=0.014018\n",
      "   XLI: DirAcc=0.5334  MAE=0.008395  RMSE=0.012315\n",
      "   XLK: DirAcc=0.5469  MAE=0.009224  RMSE=0.013457\n",
      "   XLP: DirAcc=0.5270  MAE=0.005977  RMSE=0.008693\n",
      "   XLV: DirAcc=0.5228  MAE=0.007140  RMSE=0.010181\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon  1 days: 0.525\n",
      "Mean MAE: 0.009\n",
      "Mean RMSE: 0.013\n",
      "   XLE: DirAcc=0.5186  MAE=0.026941  RMSE=0.039138\n",
      "   XLF: DirAcc=0.5510  MAE=0.020891  RMSE=0.029593\n",
      "   XLI: DirAcc=0.5727  MAE=0.019090  RMSE=0.027049\n",
      "   XLK: DirAcc=0.5889  MAE=0.020054  RMSE=0.027495\n",
      "   XLP: DirAcc=0.5640  MAE=0.012760  RMSE=0.017761\n",
      "   XLV: DirAcc=0.5637  MAE=0.015727  RMSE=0.021495\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon  5 days: 0.560\n",
      "Mean MAE: 0.019\n",
      "Mean RMSE: 0.027\n",
      "   XLE: DirAcc=0.5246  MAE=0.038303  RMSE=0.056294\n",
      "   XLF: DirAcc=0.5737  MAE=0.029123  RMSE=0.041272\n",
      "   XLI: DirAcc=0.5825  MAE=0.026313  RMSE=0.037516\n",
      "   XLK: DirAcc=0.6157  MAE=0.027968  RMSE=0.037465\n",
      "   XLP: DirAcc=0.5897  MAE=0.017790  RMSE=0.024430\n",
      "   XLV: DirAcc=0.5820  MAE=0.021708  RMSE=0.029358\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon 10 days: 0.578\n",
      "Mean MAE: 0.027\n",
      "Mean RMSE: 0.038\n",
      "   XLE: DirAcc=0.5159  MAE=0.054822  RMSE=0.081040\n",
      "   XLF: DirAcc=0.5934  MAE=0.040786  RMSE=0.058270\n",
      "   XLI: DirAcc=0.5993  MAE=0.036805  RMSE=0.053234\n",
      "   XLK: DirAcc=0.6549  MAE=0.038460  RMSE=0.051547\n",
      "   XLP: DirAcc=0.6099  MAE=0.024887  RMSE=0.033376\n",
      "   XLV: DirAcc=0.6158  MAE=0.029322  RMSE=0.039120\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon 20 days: 0.598\n",
      "Mean MAE: 0.038\n",
      "Mean RMSE: 0.053\n",
      "   XLE: DirAcc=0.5486  MAE=0.075902  RMSE=0.108377\n",
      "   XLF: DirAcc=0.6228  MAE=0.056311  RMSE=0.078351\n",
      "   XLI: DirAcc=0.6461  MAE=0.049547  RMSE=0.069802\n",
      "   XLK: DirAcc=0.6943  MAE=0.050606  RMSE=0.066963\n",
      "   XLP: DirAcc=0.6491  MAE=0.033269  RMSE=0.043040\n",
      "   XLV: DirAcc=0.6536  MAE=0.039724  RMSE=0.051297\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon 40 days: 0.636\n",
      "Mean MAE: 0.051\n",
      "Mean RMSE: 0.070\n",
      "   XLE: DirAcc=0.5667  MAE=0.085446  RMSE=0.120330\n",
      "   XLF: DirAcc=0.6530  MAE=0.062778  RMSE=0.087211\n",
      "   XLI: DirAcc=0.6704  MAE=0.054803  RMSE=0.076613\n",
      "   XLK: DirAcc=0.7182  MAE=0.054296  RMSE=0.071722\n",
      "   XLP: DirAcc=0.6543  MAE=0.036285  RMSE=0.046005\n",
      "   XLV: DirAcc=0.6710  MAE=0.043152  RMSE=0.055600\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon 50 days: 0.656\n",
      "Mean MAE: 0.056\n",
      "Mean RMSE: 0.076\n",
      "   XLE: DirAcc=0.5722  MAE=0.097521  RMSE=0.137108\n",
      "   XLF: DirAcc=0.6608  MAE=0.074315  RMSE=0.100041\n",
      "   XLI: DirAcc=0.6897  MAE=0.064047  RMSE=0.087253\n",
      "   XLK: DirAcc=0.7497  MAE=0.060497  RMSE=0.079659\n",
      "   XLP: DirAcc=0.7024  MAE=0.038921  RMSE=0.049573\n",
      "   XLV: DirAcc=0.7151  MAE=0.047315  RMSE=0.060413\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon 70 days: 0.682\n",
      "Mean MAE: 0.064\n",
      "Mean RMSE: 0.086\n",
      "   XLE: DirAcc=0.5922  MAE=0.114072  RMSE=0.160725\n",
      "   XLF: DirAcc=0.6850  MAE=0.088945  RMSE=0.117257\n",
      "   XLI: DirAcc=0.7043  MAE=0.072597  RMSE=0.098529\n",
      "   XLK: DirAcc=0.7977  MAE=0.069487  RMSE=0.094138\n",
      "   XLP: DirAcc=0.7337  MAE=0.042896  RMSE=0.054789\n",
      "   XLV: DirAcc=0.7223  MAE=0.053918  RMSE=0.067193\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon 100 days: 0.706\n",
      "Mean MAE: 0.074\n",
      "Mean RMSE: 0.099\n"
     ]
    }
   ],
   "source": [
    "# Normal Student\n",
    "\n",
    "from mltester import run_mltester\n",
    "mean_diraccuracies_student = []\n",
    "mean_mae_student = []\n",
    "mean_rmse_student = []\n",
    "horizons = [1, 5, 10, 20, 40, 50, 70, 100]\n",
    "\n",
    "for horizon in horizons:\n",
    "    results = run_mltester(\n",
    "        model_spec=\"student.py:Student\",  \n",
    "        tickers=[\"XLE\", \"XLF\", \"XLI\", \"XLK\", \"XLP\", \"XLV\"],            \n",
    "        data_file=\"prices.csv\",\n",
    "        horizon=horizon,\n",
    "        step=10,                           \n",
    "    )\n",
    "    mean_diraccuracy = results['diracc'].iloc[-1]  \n",
    "    mean_diraccuracies_student.append(mean_diraccuracy)\n",
    "    print(f\"Horizon {horizon:2d} days: {mean_diraccuracy:.3f}\")\n",
    "\n",
    "    mean_mae = results['mae'].iloc[-1] \n",
    "    mean_mae_student.append(mean_mae)\n",
    "    print(f\"Mean MAE: {mean_mae:.3f}\")\n",
    "\n",
    "    mean_rmse = results['rmse'].iloc[-1]  \n",
    "    mean_rmse_student.append(mean_rmse)\n",
    "    print(f\"Mean RMSE: {mean_rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670be434",
   "metadata": {},
   "source": [
    "\n",
    "To integrate my initial student.py from Coursework part 1 with Kmeans; I extended the class (please see student_ext.py) by creating the external file (via KMeans_Data_Prep.py file, \"detected_regimes_k{K}.csv\", e.g. detected_regimes_k2.csv) and making it required in my student_ext.py (line 28)\n",
    "\n",
    "Within my extended student, I've made it so that the columns; date, regime and regime_lag1 are required (for no data leakage, lines 50-55), input is the X dataframe (same as the original code) and also my detected_regimes_k2.csv. DateTimeIndex is sorted because time series operations like .loc[date] and shift() assume chronological order (line 60).\n",
    "\n",
    "For the _make_features() function in my extended student, it now has 13 features instead of 12 (original part 1 student features with regime column, line 179). Helper methods are same as my original part 1 student code (lines 93-121). \n",
    "\n",
    "My extended student has a new function; _get_regime_for_date(self, date) (lines 126-140); this is a time-safe regime lookup function using the lagged regime data (line 129), again to ensure no data leakage (by ensuring regime at time t uses information up to t-1) (lines 63, 64 and line 129) and includes some handling of edge cases, such as NaN values (line 134). \n",
    "\n",
    "For the fit() function, I've added one-hot encoding of regime labels (lines 200-203), creating regime-specific indicator features. Additionally, I store the training column strucutre to ensure consistent feature alignment during prediction (line 206).  \n",
    "\n",
    "For predict() I have added column alignment logic (lines 278-288) to ensure prediction features match the training structure, including creating one-hot regime columns for new data and reordering columns to maintain consistency with training phase.\n",
    "\n",
    "Compared to my original student, my extended student contains error-handling, such as validating that  the regime file has required columns (lines 50-56), handles regimes not seen during training (line 287), aligns prediction features to training structures and manages int64 nullable types (lines 68, 69, 200, 275) and is overall more robust than my original student.py class. \n",
    "\n",
    "Within my kmeans student class, I've also embedded the core functions of mltester; specifically forward_log_return(), compute_metrics(), walk_forward_predict(), evaluate_ticker(). Within my walk_forward_predict() (compared to the original mltester.py) I made it so that each walk-forward block gets a fresh model with the same regime file. I've also simplified data loading, compared to the original mltester.py (lines 396, 407).\n",
    "\n",
    "Looking at the results of the given DirAcc, MAE and RMSE from different K values, it successfully demonstrates that regime detection works due to differenet results, and as expected, K=2 performs best as it has better DirAcc and lower MAE/RMSE on average compared to K=3 and K=4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea28e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying all the differnt k's to verify silhouette/CH findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7e334",
   "metadata": {},
   "source": [
    "### Experimentation of Different Methods within my KMeans clustering.\n",
    "\n",
    "As of right now, my Kmeans clustering file uses;\n",
    "- StandardScaler(),\n",
    "- No PCA to generate clusters,\n",
    "- n_init = 50,\n",
    "\n",
    "In an attempt to improve the Directional Accuracy and decrease the MAE and RMSE values; I will try:\n",
    "- changing the StandardScaler() to RobustScaler(),\n",
    "- chanding the StandardScaler() to MinMaxScaler(),\n",
    "- implement PCA within the cluster generation and \n",
    "- changing the n_init value to 20 (between 20-50, as per ISLP's guidance). \n",
    "- changing the n_init value to 30 \n",
    "\n",
    "\n",
    "Below I've detailed my experimentation journey for each experiment method. \n",
    "\n",
    "#### RobustScaler (aka Robust):\n",
    "\n",
    "Within my KMeans_Data_Prep, I changed the scaler used on line 42 from StandardScaler() to RobustScaler() (Appendix/Experimentation/KMeans_Data_Prep_robustScaler). And changed the scaler used in the Appendix/Experimentation/student_kmeans_multvariate_robust.py which runs the mltester functionality on my computed timeseries. I decided to try replacing StandardScaler() with RobustScaler() as StandardScaler() (according to GeeksForGeeks: https://www.geeksforgeeks.org/machine-learning/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/ ), \"subtracts the mean of the data and divides it by standard deviation. This centers the data around zero and standardizes the variablity\". It is more sensitive to outliers. Whereas RobustScaler \"subtracts the median of data and divides by interquartile range (IQR) which helps in reducing the effect of outliers why maintaining distribution of non-outlier values.\"\n",
    "\n",
    "#### PCA:\n",
    "Within my KMeans_Data_Prep, I've added PCA (principle component analysis) for dimensionality reduction and to help \"reduce the number of features in a dataset while keeping the most important information\"  according to GeeksForGeeks (https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/ ). This in itself can lead to less noise and redundant information, resulting in cleaner, more stable clusters. PCA also looks to resolve the curse of dimensionality , as in high-dimensional spaces data can become sparse and can lead to an overfitting risk.\n",
    "\n",
    "#### n_init20 (aka Init20) and n_init30 (aka Init30)\n",
    "Within my KMeans_Data_Prep, I've changed the n_init value from 50 to 20 and 30 by changing the n_init parameter within the kmeans initialisation (line 55).\n",
    "The n_init parameter is defined as the \"Number of times the k-means algorithm is run with different centroid seeds\" according to the scikit-learn docs (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html ). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1425f1f",
   "metadata": {},
   "source": [
    "#### Directional Accuracy Results\n",
    "\n",
    "![directional_accuracy_zoomed.png](method_comparison/directional_accuracy_zoomed1.png)\n",
    "(The above graph is zoomed in for easier distinction between bars)\n",
    "\n",
    "\n",
    "\n",
    "From the results; \n",
    "- Both the Init20 and Init30 have the same averages as each other across the differing k values, indicating that changing the n_init value does not make a difference to the directional accuracy.\n",
    "- For PCA addtion in regime detection; it has a high DirAcc in k=2 and k=3, then second highest in k=4, but stil higher than my original kmeans code. \n",
    "- For RobustScaler; it is the lowest scoring method for DirAcc in k=2, the same as other  for k=3 but has the highest score for k=4.\n",
    "- For my original student kmeans, it has a high DirAcc in k=2 and k=3, but then has the lowest DirAcc overall in K=4, indicating that my baseline clustering has become unstable at higher K values.\n",
    "\n",
    "Worst performing was Robust in K=2, then for k=4 it was init20, init 30 and my kmeans student.\n",
    "\n",
    "Overall I have found that regimes can improve predictional accuracy. For K=2, the highest directional accuracy is given by init20, init30, PCA and my original kmeans student. DirAcc is the same across all methods at k=3 and then Robust has the highest DirAcc at K=4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbdb5d",
   "metadata": {},
   "source": [
    "#### MAE Results\n",
    "\n",
    "![mae_zoomed.png](method_comparison/mae_zoomed1.png)\n",
    "(The above graph is zoomed in for easier distinction between bars)\n",
    "\n",
    "From the results:\n",
    "- Similar to DirAcc, both the Init20 and Init30 have the same averages for MAE as each other for the differing K values, indicating that changing the n_init value does not make a difference to MAE score.\n",
    "- PCA has the third highest MAE score for k=2, then the highest MAE for k=3, then second lowest for k=4.\n",
    "- Robust has the lowest MAE score overall at all k values.\n",
    "- My original kmeans student has the highest MAE at k=2, second highest at k=3 and has the same MAE value as init20 and init30 at k=4.\n",
    "\n",
    "Overall, for minimising the MAE score, RobustScaler() performed best in all k values. Worst performing (as in, highest scoring) was my student kmeans in k=2, PCA for k=3, then init20, init30 and my student kmeans for k=4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4afcff",
   "metadata": {},
   "source": [
    "#### RMSE Results\n",
    "\n",
    "![rmse_zoomed.png](method_comparison/rmse_zoomed1.png)\n",
    "(The above graph is zoomed in for easier distinction between bars)\n",
    "\n",
    "Results are very similar to results in MAE in regards to output. Overall, for minimising the RMSE score, RobustScaler() performed best in all k values. Worst performing (as in, highest scoring) was my student kmeans in k=2, PCA for k=3, then init20, init30 and my student kmeans for k=4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b430c8c",
   "metadata": {},
   "source": [
    "#### Decisions made based on results\n",
    "\n",
    "Based upon the results of my experimentation, I must consider the trade-off of having higher directional accuracy with higher MAE and RMSE, or slightly lower directional accuracy with lower MAE and RMSE. As RobustScaler() has shown to be stable throughout DirAcc, MAE and RMSE results, (especially considering the difference between the highest scoring methods for K=2 and Robust was 0.0008), I have decided to implement it within my student_ext.py. \n",
    "\n",
    "To run my student_ext.py, I will first prepare the data (now with using RobustScaler file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c4b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "K = 2\n",
      "Silhouette: 0.334 | CH: 144.5\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0075, vol=0.0114 -> BULL\n",
      "Cluster 1: ret=-0.0207, vol=0.0137 -> BEAR\n",
      "Saved: detected_regimes_k2_robustscaler.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 3\n",
      "Silhouette: 0.296 | CH: 142.5\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0211, vol=0.0137 -> BULL\n",
      "Cluster 1: ret=-0.0294, vol=0.0140 -> BEAR\n",
      "Cluster 2: ret=0.0035, vol=0.0109 -> NEUTRAL\n",
      "Saved: detected_regimes_k3_robustscaler.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 4\n",
      "Silhouette: 0.154 | CH: 122.5\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=-0.0067, vol=0.0111 -> NEUTRAL\n",
      "Cluster 1: ret=0.0195, vol=0.0141 -> NEUTRAL\n",
      "Cluster 2: ret=0.0115, vol=0.0106 -> BULL\n",
      "Cluster 3: ret=-0.0361, vol=0.0158 -> BEAR\n",
      "Saved: detected_regimes_k4_robustscaler.csv\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run DataPreparation/KMeans_Data_Prep_robustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING K=2 REGIMES\n",
      "==================================================\n",
      "\n",
      "Evaluating horizon 1 days...\n",
      "   XLE: DirAcc=0.5127  MAE=0.011796  RMSE=0.017381\n",
      "   XLF: DirAcc=0.5148  MAE=0.009424  RMSE=0.013988\n",
      "   XLI: DirAcc=0.5376  MAE=0.008376  RMSE=0.012288\n",
      "   XLK: DirAcc=0.5490  MAE=0.009201  RMSE=0.013440\n",
      "   XLP: DirAcc=0.5323  MAE=0.005938  RMSE=0.008649\n",
      "   XLV: DirAcc=0.5281  MAE=0.007118  RMSE=0.010159\n",
      "Horizon   1 days: DirAcc=0.529 | MAE=0.0086 | RMSE=0.0127\n",
      "\n",
      "Evaluating horizon 5 days...\n",
      "   XLE: DirAcc=0.5305  MAE=0.026839  RMSE=0.039030\n",
      "   XLF: DirAcc=0.5568  MAE=0.020812  RMSE=0.029535\n",
      "   XLI: DirAcc=0.5807  MAE=0.019007  RMSE=0.026991\n",
      "   XLK: DirAcc=0.5958  MAE=0.020012  RMSE=0.027484\n",
      "   XLP: DirAcc=0.5791  MAE=0.012665  RMSE=0.017620\n",
      "   XLV: DirAcc=0.5714  MAE=0.015699  RMSE=0.021462\n",
      "Horizon   5 days: DirAcc=0.569 | MAE=0.0192 | RMSE=0.0270\n",
      "\n",
      "Evaluating horizon 10 days...\n",
      "   XLE: DirAcc=0.5349  MAE=0.038098  RMSE=0.055896\n",
      "   XLF: DirAcc=0.5857  MAE=0.028969  RMSE=0.041096\n",
      "   XLI: DirAcc=0.5937  MAE=0.026170  RMSE=0.037358\n",
      "   XLK: DirAcc=0.6256  MAE=0.027848  RMSE=0.037391\n",
      "   XLP: DirAcc=0.6096  MAE=0.017613  RMSE=0.024215\n",
      "   XLV: DirAcc=0.5969  MAE=0.021666  RMSE=0.029307\n",
      "Horizon  10 days: DirAcc=0.591 | MAE=0.0267 | RMSE=0.0375\n",
      "\n",
      "Evaluating horizon 20 days...\n",
      "   XLE: DirAcc=0.5513  MAE=0.054686  RMSE=0.080323\n",
      "   XLF: DirAcc=0.6136  MAE=0.040658  RMSE=0.058044\n",
      "   XLI: DirAcc=0.6123  MAE=0.036587  RMSE=0.052929\n",
      "   XLK: DirAcc=0.6683  MAE=0.038226  RMSE=0.051351\n",
      "   XLP: DirAcc=0.6342  MAE=0.024745  RMSE=0.033222\n",
      "   XLV: DirAcc=0.6296  MAE=0.029413  RMSE=0.039270\n",
      "Horizon  20 days: DirAcc=0.618 | MAE=0.0374 | RMSE=0.0525\n",
      "\n",
      "Evaluating horizon 40 days...\n",
      "   XLE: DirAcc=0.5628  MAE=0.075476  RMSE=0.107326\n",
      "   XLF: DirAcc=0.6437  MAE=0.056200  RMSE=0.078159\n",
      "   XLI: DirAcc=0.6713  MAE=0.049333  RMSE=0.069561\n",
      "   XLK: DirAcc=0.7064  MAE=0.050482  RMSE=0.066895\n",
      "   XLP: DirAcc=0.6780  MAE=0.033325  RMSE=0.043009\n",
      "   XLV: DirAcc=0.6705  MAE=0.039773  RMSE=0.051344\n",
      "Horizon  40 days: DirAcc=0.655 | MAE=0.0508 | RMSE=0.0694\n",
      "\n",
      "Evaluating horizon 50 days...\n",
      "   XLE: DirAcc=0.5719  MAE=0.085017  RMSE=0.119038\n",
      "   XLF: DirAcc=0.6755  MAE=0.062722  RMSE=0.087044\n",
      "   XLI: DirAcc=0.6938  MAE=0.054674  RMSE=0.076470\n",
      "   XLK: DirAcc=0.7354  MAE=0.054189  RMSE=0.071614\n",
      "   XLP: DirAcc=0.6925  MAE=0.036502  RMSE=0.046255\n",
      "   XLV: DirAcc=0.6949  MAE=0.043104  RMSE=0.055568\n",
      "Horizon  50 days: DirAcc=0.677 | MAE=0.0560 | RMSE=0.0760\n",
      "\n",
      "Evaluating horizon 70 days...\n",
      "   XLE: DirAcc=0.6006  MAE=0.097168  RMSE=0.135109\n",
      "   XLF: DirAcc=0.6816  MAE=0.074465  RMSE=0.100052\n",
      "   XLI: DirAcc=0.7110  MAE=0.063786  RMSE=0.086968\n",
      "   XLK: DirAcc=0.7602  MAE=0.059973  RMSE=0.079322\n",
      "   XLP: DirAcc=0.7443  MAE=0.038907  RMSE=0.049735\n",
      "   XLV: DirAcc=0.7448  MAE=0.047223  RMSE=0.060286\n",
      "Horizon  70 days: DirAcc=0.707 | MAE=0.0636 | RMSE=0.0852\n",
      "\n",
      "Evaluating horizon 100 days...\n",
      "   XLE: DirAcc=0.6256  MAE=0.113105  RMSE=0.158020\n",
      "   XLF: DirAcc=0.7092  MAE=0.088826  RMSE=0.116786\n",
      "   XLI: DirAcc=0.7356  MAE=0.072352  RMSE=0.097667\n",
      "   XLK: DirAcc=0.8121  MAE=0.068349  RMSE=0.093145\n",
      "   XLP: DirAcc=0.7901  MAE=0.043061  RMSE=0.055151\n",
      "   XLV: DirAcc=0.7522  MAE=0.053833  RMSE=0.067126\n",
      "Horizon 100 days: DirAcc=0.737 | MAE=0.0733 | RMSE=0.0980\n",
      "\n",
      "K=2 Summary:\n",
      "Average DirAcc across horizons: 0.636\n",
      "Average MAE across horizons:    0.0419\n",
      "Average RMSE across horizons:   0.0573\n",
      "\n",
      "==================================================\n",
      "TESTING K=3 REGIMES\n",
      "==================================================\n",
      "\n",
      "Evaluating horizon 1 days...\n",
      "   XLE: DirAcc=0.5127  MAE=0.011796  RMSE=0.017381\n",
      "   XLF: DirAcc=0.5148  MAE=0.009424  RMSE=0.013988\n",
      "   XLI: DirAcc=0.5376  MAE=0.008376  RMSE=0.012288\n",
      "   XLK: DirAcc=0.5490  MAE=0.009201  RMSE=0.013440\n",
      "   XLP: DirAcc=0.5323  MAE=0.005938  RMSE=0.008649\n",
      "   XLV: DirAcc=0.5281  MAE=0.007118  RMSE=0.010159\n",
      "Horizon   1 days: DirAcc=0.529 | MAE=0.0086 | RMSE=0.0127\n",
      "\n",
      "Evaluating horizon 5 days...\n",
      "   XLE: DirAcc=0.5305  MAE=0.026839  RMSE=0.039030\n",
      "   XLF: DirAcc=0.5571  MAE=0.020811  RMSE=0.029535\n",
      "   XLI: DirAcc=0.5807  MAE=0.019007  RMSE=0.026991\n",
      "   XLK: DirAcc=0.5958  MAE=0.020012  RMSE=0.027484\n",
      "   XLP: DirAcc=0.5791  MAE=0.012665  RMSE=0.017620\n",
      "   XLV: DirAcc=0.5714  MAE=0.015699  RMSE=0.021462\n",
      "Horizon   5 days: DirAcc=0.569 | MAE=0.0192 | RMSE=0.0270\n",
      "\n",
      "Evaluating horizon 10 days...\n",
      "   XLE: DirAcc=0.5349  MAE=0.038086  RMSE=0.055881\n",
      "   XLF: DirAcc=0.5857  MAE=0.028969  RMSE=0.041096\n",
      "   XLI: DirAcc=0.5937  MAE=0.026166  RMSE=0.037355\n",
      "   XLK: DirAcc=0.6256  MAE=0.027849  RMSE=0.037392\n",
      "   XLP: DirAcc=0.6096  MAE=0.017613  RMSE=0.024215\n",
      "   XLV: DirAcc=0.5969  MAE=0.021666  RMSE=0.029307\n",
      "Horizon  10 days: DirAcc=0.591 | MAE=0.0267 | RMSE=0.0375\n",
      "\n",
      "Evaluating horizon 20 days...\n",
      "   XLE: DirAcc=0.5500  MAE=0.054542  RMSE=0.080139\n",
      "   XLF: DirAcc=0.6136  MAE=0.040666  RMSE=0.058059\n",
      "   XLI: DirAcc=0.6120  MAE=0.036612  RMSE=0.052968\n",
      "   XLK: DirAcc=0.6683  MAE=0.038154  RMSE=0.051243\n",
      "   XLP: DirAcc=0.6342  MAE=0.024744  RMSE=0.033221\n",
      "   XLV: DirAcc=0.6296  MAE=0.029407  RMSE=0.039269\n",
      "Horizon  20 days: DirAcc=0.618 | MAE=0.0374 | RMSE=0.0525\n",
      "\n",
      "Evaluating horizon 40 days...\n",
      "   XLE: DirAcc=0.5626  MAE=0.075549  RMSE=0.107418\n",
      "   XLF: DirAcc=0.6496  MAE=0.056142  RMSE=0.078110\n",
      "   XLI: DirAcc=0.6708  MAE=0.049391  RMSE=0.069653\n",
      "   XLK: DirAcc=0.7035  MAE=0.050969  RMSE=0.067289\n",
      "   XLP: DirAcc=0.6780  MAE=0.033303  RMSE=0.042979\n",
      "   XLV: DirAcc=0.6732  MAE=0.039702  RMSE=0.051295\n",
      "Horizon  40 days: DirAcc=0.656 | MAE=0.0508 | RMSE=0.0695\n",
      "\n",
      "Evaluating horizon 50 days...\n",
      "   XLE: DirAcc=0.5719  MAE=0.085125  RMSE=0.119130\n",
      "   XLF: DirAcc=0.6782  MAE=0.062464  RMSE=0.086825\n",
      "   XLI: DirAcc=0.6925  MAE=0.055590  RMSE=0.077415\n",
      "   XLK: DirAcc=0.7287  MAE=0.055426  RMSE=0.072867\n",
      "   XLP: DirAcc=0.6925  MAE=0.036427  RMSE=0.046202\n",
      "   XLV: DirAcc=0.6973  MAE=0.043048  RMSE=0.055546\n",
      "Horizon  50 days: DirAcc=0.677 | MAE=0.0563 | RMSE=0.0763\n",
      "\n",
      "Evaluating horizon 70 days...\n",
      "   XLE: DirAcc=0.6009  MAE=0.097177  RMSE=0.135172\n",
      "   XLF: DirAcc=0.6959  MAE=0.074137  RMSE=0.099963\n",
      "   XLI: DirAcc=0.7154  MAE=0.063800  RMSE=0.087121\n",
      "   XLK: DirAcc=0.7640  MAE=0.061472  RMSE=0.081322\n",
      "   XLP: DirAcc=0.7443  MAE=0.038860  RMSE=0.049725\n",
      "   XLV: DirAcc=0.7448  MAE=0.047215  RMSE=0.060281\n",
      "Horizon  70 days: DirAcc=0.711 | MAE=0.0638 | RMSE=0.0856\n",
      "\n",
      "Evaluating horizon 100 days...\n",
      "   XLE: DirAcc=0.6199  MAE=0.113029  RMSE=0.157954\n",
      "   XLF: DirAcc=0.6973  MAE=0.089769  RMSE=0.118015\n",
      "   XLI: DirAcc=0.7275  MAE=0.073248  RMSE=0.098492\n",
      "   XLK: DirAcc=0.8100  MAE=0.069009  RMSE=0.093814\n"
     ]
    }
   ],
   "source": [
    "%run student_ext.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: regimes_mcgreevy_weekly.csv\n"
     ]
    }
   ],
   "source": [
    "%run PublishedComparison/ComputeRegimes_McGreevy.py\n",
    "\n",
    "# this creates regimes_mcgreevy_weekly.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b12ad",
   "metadata": {},
   "source": [
    "## 4. Baseline Comparison\n",
    "\n",
    "For baseline comparison, I will be using the \"Detecting Multivariate Market Regimes via Clustering Algorithms\" method (2024) by James McGreevy (Aitor Muguruza-Gonzalez, Zacharia Issa Jonathan Chan and Cris Salvi).\n",
    "\n",
    "\n",
    "Key takeaways of the McGreevy et al paper is as follows:\n",
    "\n",
    "\"• We develop an adapted k-means algorithm that uses the 2-Wasserstein distance metric or Max\u0002imum Mean Discrepancy, and d-dimensional data in order to identify changes in joint market\n",
    "regimes between assets, in particular correlation.\n",
    "• We create a two-step process for finding the marginal and joint market regimes in synthetic and\n",
    "real data.\n",
    "• Using the two-step process, we form approximations to the mean, variance and correlation which\n",
    "then subsequently inform profitable portfolios of pairs of stocks.\"\n",
    "\n",
    "To begin creating my version of McGreevy et al for this assignment, I first created a script to compute regimes, then also created the class containing the fit() and predict() functions that uses the computed regimes. For McGreevy et al (2024), the paper's main contribution is showing how it adapts clustering for the specific problem - using a probability metric (specifically; 2-Wasserstein distance between empirical measures) within it's clustering framework to compare the data segment distributions, rather than using raw data points. More specifically, the novelty of McGreevy et al's is replacing Euclidean distance with Wasserstein to compare data segment distributions.\n",
    "\n",
    "### Creation of the Regimes (ComputeRegimes_McGreevy.py)\n",
    "\n",
    "\n",
    "For McGreevy et al, it states: \"Therefore,\n",
    "we may rewrite the goal of the MRCP (market regime clustering problem) as being equivalent to assigning labels to empirical probability measures µ ∈ P_p(R^d), where P_p(R^d) is the set of probability measures on R^d with finite pth moment.\n",
    "In practice we wish to take potentially overlapping segments of returns data for d assets and gather the empirical measures associated to these segments together into distinct clusters, each defined by an\n",
    "underlying distribution, which we then refer to as market regimes.\"  For my preparation of the data, I use:\n",
    "\n",
    "segs, seg_dates = segment(returns_z, H1, H2)\n",
    "\n",
    "\n",
    "To segment the lenght of h1, overlapping h2.\n",
    "\n",
    "![h1_minus_h2_block_mcgreevy.png](Images/h1_minus_h2_block_mcgreevy.png)\n",
    "\n",
    "(Image is taken from another McGreevy et al paper (2023)).\n",
    "\n",
    "Figure 5.2 is a horizontal timeline of returns divided into boxes (where each box = an observation, e.g. one week). h_1 represents the segment length (e.g. from the image; 5 boxes = 5 weeks). h_2 represents the overlap between consecutive segments (e.g. spanning all 5 boxes / weeks). The red block indicates the current segment being clustered. The segments (\\mu_1, \\mu_2, e.t.c.) represents an empirical distribution of returns over h_1 weeks, with the lower arrows showing an overlapping sliding forward windows by h_1 - h_2 steps. I attempt to recreate this in my segment function.\n",
    "\n",
    "Within my segment function, I create overlapping windows of length h1 (20 weeks in my configuration). For my segments; each new segment starts h1-h2 weeks after previous one (20-16= 4); where h1 is the full segment and h2 is the overlap. I create a matrix of returns for six tickers over h1. The dates variable acts as an anchor for the next interval. \n",
    "\n",
    "For preprocessing, I chose weekly resampling, to keep it in line with my current data preprocessing for this assignment. I create a wide panel (1 column per ticker) to implement the multivariate setting, using k=2 as mentioned in the McGreevy paper: \"The simplest possible choice is k = 2\", and seed kept at 42 to keep it the same throughout my assignment. \n",
    "\n",
    "\n",
    "Within my code (appendix/PublishedComparision/ComputeRegimes_McGreevy.py), I have my segment function:\n",
    "\n",
    "\"def segment(R, h1, h2):\n",
    "    step = h1 - h2\n",
    "    segs = [R.iloc[i-h1+1:i+1].values for i in range(h1-1, len(R)-1, step)]\n",
    "    dates = [R.index[i+1] for i in range(h1-1, len(R)-1, step)]\n",
    "    return segs, pd.DatetimeIndex(dates)\"\n",
    "\n",
    "The purpose of this segment function is to create the foundation of the published method; specifically by dividing the multivariate timeseries into overlapping rolling windows for clustering. Within this I use the parameters H1 and H2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382e413",
   "metadata": {},
   "source": [
    "![h1_minus_h2_block_mcgreevy.png](Images/h1_minus_h2_block_mcgreevy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c05f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----------------------------------------------\n",
    "def w2(X, Y):\n",
    "    if X.shape[0] != Y.shape[0]:\n",
    "        m = min(X.shape[0], Y.shape[0])\n",
    "        X, Y = X[:m], Y[:m]\n",
    "    \n",
    "    if X.shape[1] == 1:\n",
    "        return np.sqrt(np.mean((np.sort(X[:, 0]) - np.sort(Y[:, 0]))**2))\n",
    "    \n",
    "    C = np.sum((X[:, None, :] - Y[None, :, :])**2, axis=2)\n",
    "    r, c = linear_sum_assignment(C)\n",
    "    return np.sqrt(C[r, c].mean())\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "For my w2(X, Y) function, it is the Wasserstein-2 distance between empiracl measures. I use Wasserstein as the paper emphasises it's use as a core metric for computing the distance between 2 empirical distributions, e.g. in the Abstract: \"In particular, we empirically investigate the performance of the algorithm endowed with either Wasserstein distances or Maximum Mean Discrepancies\". Overall, the function w2(X,Y) implements the 2-Wasserstein distance between the two empirical segment distributions (X and Y). The Wasserstein distance is in 1-D using the quantile closed-form (Section 4.1.1 [1]). The 'C' variable builds the cost matrix of squared Euclidean distances. THe 'r, c' variables run the Hungarian algorithm on 'C' (the linear_sum_assignmnet), and then returns the final distance by taking the square root of the mean of the optimal assignment costs.\n",
    "\n",
    "For my wkmeans(D, k, prev_centres=None, seed=42) function, I run a kmeans style loop on a precomputed distance matrix (D), which is entries of 2-Wasserstein distances between empirical measures (segments). I then assign each segment to it's closest centre under 2-Wasserstein. \n",
    "\n",
    "I then update the step (barycentre approximation: \" \n",
    "\n",
    "intra = D[np.ix_(idx, idx)].sum(1)\n",
    "new_centres.append(int(idx[int(np.argmin(intra))]))\"\n",
    "                \n",
    ", as per paper: \"Furthermore, the concept of an average or central measure amongst a set of measures has a natural form when using the p-Wasserstein distance. This is called the Wasserstein barycentre\") for each cluster - choosing the new centre as the medoid. The medoid is defined as \"the most centrally located data point within a cluster.\" According to GeeksForGeeks (https://www.geeksforgeeks.org/machine-learning/k-medoids-clustering-in-machine-learning/ )\n",
    "\n",
    "Although specifically not detailed within McGreevy et al, I included empty cluster handling (\"if len(idx) == 0: ...\" - if idx is empty, there are no members in the cluster, my if statement reseeds the centre so the algorithm can continue). \n",
    "\n",
    "Mentioned within the McGreevy baseline is the two-step Copula variant: \"Step 2: Multivariate clustering\n",
    "The joint distribution of our newly transformed dataset should be the copula of the original data, and so we apply the 2-d WK-means or 2-d MMDK-means algorithm to our transformed data in order to obtain correlation regimes\" . I was not able to code this due to it's complexity, so my ComputeRegimes_McGreevy.py includes the core single-stage clustering piece withn my wkmeans().\n",
    "\n",
    "Also within my wkmeans()  I implement label persistence by building a cost matrix between previous and current centres, finding the minimum cost between old and new centres (Hungarian), creating a remap from new centre indext to old centre index, remap point labels so they refer to the same 'identity' as previous iteration and reorder centres to match previous identity order:\n",
    "\n",
    "    if prev_centres is not None:\n",
    "        cost = D[np.ix_(prev_centres, centres)]\n",
    "        r, c = linear_sum_assignment(cost)\n",
    "        remap = {new: old for old, new in zip(r, c)}\n",
    "        labels = np.array([remap.get(L, L) for L in labels], dtype=int)\n",
    "        centres = centres[c]\n",
    "    \n",
    "    return labels, centres\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb211b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   XLE: DirAcc=0.4828  MAE=0.011843  RMSE=0.017445\n",
      "   XLF: DirAcc=0.4910  MAE=0.009461  RMSE=0.014030\n",
      "   XLI: DirAcc=0.5130  MAE=0.008410  RMSE=0.012329\n",
      "   XLK: DirAcc=0.5239  MAE=0.009236  RMSE=0.013464\n",
      "   XLP: DirAcc=0.5095  MAE=0.005978  RMSE=0.008695\n",
      "   XLV: DirAcc=0.5141  MAE=0.007143  RMSE=0.010195\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon   1 days: DirAcc=0.506 | MAE=0.0087 | RMSE=0.0127\n",
      "   XLE: DirAcc=0.4841  MAE=0.026992  RMSE=0.039153\n",
      "   XLF: DirAcc=0.5080  MAE=0.021022  RMSE=0.029708\n",
      "   XLI: DirAcc=0.5422  MAE=0.019263  RMSE=0.027131\n",
      "   XLK: DirAcc=0.5653  MAE=0.020094  RMSE=0.027505\n",
      "   XLP: DirAcc=0.5507  MAE=0.012726  RMSE=0.017750\n",
      "   XLV: DirAcc=0.5547  MAE=0.015713  RMSE=0.021542\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon   5 days: DirAcc=0.534 | MAE=0.0193 | RMSE=0.0271\n",
      "   XLE: DirAcc=0.4956  MAE=0.038307  RMSE=0.056330\n",
      "   XLF: DirAcc=0.5323  MAE=0.029452  RMSE=0.041467\n",
      "   XLI: DirAcc=0.5562  MAE=0.026547  RMSE=0.037557\n",
      "   XLK: DirAcc=0.5958  MAE=0.027881  RMSE=0.037462\n",
      "   XLP: DirAcc=0.5759  MAE=0.017695  RMSE=0.024383\n",
      "   XLV: DirAcc=0.5735  MAE=0.021636  RMSE=0.029415\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon  10 days: DirAcc=0.555 | MAE=0.0269 | RMSE=0.0378\n",
      "   XLE: DirAcc=0.5009  MAE=0.054712  RMSE=0.080896\n",
      "   XLF: DirAcc=0.5713  MAE=0.041390  RMSE=0.058235\n",
      "   XLI: DirAcc=0.5718  MAE=0.037268  RMSE=0.053163\n",
      "   XLK: DirAcc=0.6307  MAE=0.038211  RMSE=0.051340\n",
      "   XLP: DirAcc=0.5902  MAE=0.024737  RMSE=0.033325\n",
      "   XLV: DirAcc=0.6059  MAE=0.029106  RMSE=0.039224\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon  20 days: DirAcc=0.578 | MAE=0.0376 | RMSE=0.0527\n",
      "   XLE: DirAcc=0.5395  MAE=0.076090  RMSE=0.109517\n",
      "   XLF: DirAcc=0.6169  MAE=0.056246  RMSE=0.077662\n",
      "   XLI: DirAcc=0.6110  MAE=0.050684  RMSE=0.069835\n",
      "   XLK: DirAcc=0.6764  MAE=0.050314  RMSE=0.066987\n",
      "   XLP: DirAcc=0.6351  MAE=0.033053  RMSE=0.042961\n",
      "   XLV: DirAcc=0.6614  MAE=0.038347  RMSE=0.050717\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon  40 days: DirAcc=0.623 | MAE=0.0508 | RMSE=0.0696\n",
      "   XLE: DirAcc=0.5490  MAE=0.085969  RMSE=0.121527\n",
      "   XLF: DirAcc=0.6433  MAE=0.062675  RMSE=0.086340\n",
      "   XLI: DirAcc=0.6379  MAE=0.056308  RMSE=0.076916\n",
      "   XLK: DirAcc=0.7056  MAE=0.054542  RMSE=0.072234\n",
      "   XLP: DirAcc=0.6411  MAE=0.036017  RMSE=0.045880\n",
      "   XLV: DirAcc=0.6798  MAE=0.041789  RMSE=0.055045\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon  50 days: DirAcc=0.643 | MAE=0.0562 | RMSE=0.0763\n",
      "   XLE: DirAcc=0.5501  MAE=0.099165  RMSE=0.137387\n",
      "   XLF: DirAcc=0.6484  MAE=0.074182  RMSE=0.099013\n",
      "   XLI: DirAcc=0.6568  MAE=0.064067  RMSE=0.086092\n",
      "   XLK: DirAcc=0.7302  MAE=0.059491  RMSE=0.079102\n",
      "   XLP: DirAcc=0.6830  MAE=0.038293  RMSE=0.049182\n",
      "   XLV: DirAcc=0.7129  MAE=0.045737  RMSE=0.059589\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon  70 days: DirAcc=0.664 | MAE=0.0635 | RMSE=0.0851\n",
      "   XLE: DirAcc=0.5483  MAE=0.116720  RMSE=0.161170\n",
      "   XLF: DirAcc=0.6616  MAE=0.089349  RMSE=0.116712\n",
      "   XLI: DirAcc=0.6700  MAE=0.075403  RMSE=0.098251\n",
      "   XLK: DirAcc=0.7757  MAE=0.067800  RMSE=0.090870\n",
      "   XLP: DirAcc=0.7071  MAE=0.042706  RMSE=0.055836\n",
      "   XLV: DirAcc=0.7122  MAE=0.052659  RMSE=0.066322\n",
      "Saved outputs to C:\\Users\\40261885\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\outputs\n",
      "Horizon 100 days: DirAcc=0.679 | MAE=0.0741 | RMSE=0.0982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mltester import run_mltester\n",
    "\n",
    "mean_diraccuracies_mcgreevy = []\n",
    "mean_mae_mcgreevy = []\n",
    "mean_rmse_mcgreevy = []\n",
    "horizons = [1, 5, 10, 20, 40, 50, 70, 100]\n",
    "\n",
    "for horizon in horizons:\n",
    "    results = run_mltester(\n",
    "        model_spec=\"PublishedComparison/mcgreevy_baseline.py:McGreevyBaseline\",\n",
    "        tickers=[\"XLE\", \"XLF\", \"XLI\", \"XLK\", \"XLP\", \"XLV\"],\n",
    "        data_file=\"prices.csv\",\n",
    "        horizon=horizon,\n",
    "        step=10,\n",
    "    )\n",
    "    diracc = results['diracc'].iloc[-1]\n",
    "    mae    = results['mae'].iloc[-1]\n",
    "    rmse   = results['rmse'].iloc[-1]\n",
    "\n",
    "    mean_diraccuracies_mcgreevy.append(diracc)\n",
    "    mean_mae_mcgreevy.append(mae)\n",
    "    mean_rmse_mcgreevy.append(rmse)\n",
    "\n",
    "    print(f\"Horizon {horizon:3d} days: DirAcc={diracc:.3f} | MAE={mae:.4f} | RMSE={rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af785f1",
   "metadata": {},
   "source": [
    "References:\n",
    "[1] Detecting Multivariate Market Regimes via Clustering Algorithms, James Mc Greevy, Aitor Muguruza Gonzalez, Zacharia Issa, Jonathan Chan, Cris Salvi (2022-2023) Imperial College London\n",
    "\n",
    "\n",
    "[2] Horvath, Issa & Muguruza (2021); Clustering Market Regimes Using the Wasserstein Distance\n",
    "\n",
    "\n",
    "[3] An Introduction to Statisitcal Learning; Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Johnathan Taylor (2023)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
