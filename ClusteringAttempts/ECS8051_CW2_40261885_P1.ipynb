{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a504eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(\"\"\"pandas>=1.5.0\n",
    "numpy>=1.21.0\n",
    "scikit-learn>=1.0.0\n",
    "matplotlib>=3.5.0\n",
    "seaborn>=0.11.0\n",
    "xgboost>=1.7.6\n",
    "yfinance>=0.2.0\n",
    "sklearn>=1.6.1            \n",
    "statsmodels>=0.13.0\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba11d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Type\n",
    "\n",
    "# Python 3.13.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8807081",
   "metadata": {},
   "source": [
    "## NOTE; HOW TO RUN CODE/NOTEBOOK FOR ASSESSOR:\n",
    "\n",
    "**Use of Artificial Intelligence Acknowledgement: Some Files within this assignment have been created with the assistance of Gen Artificial Intelligence - indicated by a top comment**\n",
    "\n",
    "### Option 1: Estimated Completion Time: 122 minutes\n",
    "If you wish to run the full notebook, you can do so via the \"Run All\" button on the very top. This includes all horizons (horizons = [1, 5, 10, 20, 40, 50, 70, 100]) for student_ext.py and baseline comparison with McGreevy et al (2024).\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "\n",
    "### Option 2: Estimated Completion Time: 10-15 mins (for one horizon)\n",
    "If you wish to run only the required code on a specified horizon; run the first two Python cells in this notebook and search (Ctrl+F) in this notebook for **%run DataPreparation/KMeans_Data_Prep_robustScaler.py** and run the cell, then search for **%run student_ext.py** - change the \"horizons = [1, 5, 10, 20, 40, 50, 70, 100]\" line (line 380)  to a horizon of your choosing, for example; \"horizons = [10]\" and run this cell. \n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "To save on time, I have included my part 1 results, my original clustering model, experimentations of my clustering model as .csv files and I have imported their findings into this notebook. \n",
    "\n",
    "Please note: the plots presented in this notebook are saved externally in Appendix/Images... - so that they can be viewed without having to wait for the entire notebook to run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f56e1",
   "metadata": {},
   "source": [
    "# Part 2: Multivariate Regime Detection and Predictive Modelling\n",
    "\n",
    "Within this project, I will be performing the following:\n",
    "\n",
    "#### 1. Data Preparation:\n",
    "\"Using the same set of tickers as in Part 1 (XLK, XLP, XLV, XLF, XLE, XLI) to construct a multivariate timeseries from daily or weekly prices or other derived features (such as returns and volatility).\"\n",
    "\n",
    "For this assignment, I have decided upon using weekly adj_close prices. I used weekly data for clustering as:\n",
    "- weekly data has less noise compared to daily data, \n",
    "- rolling stats can be more stable on weekly data, compared to more volatile daily data,\n",
    "- less computational cost, leading to faster clustering\n",
    "\n",
    "Within this assignment, I use adj_close as it accounts for dividends, corporate actions and splits - which leads to better analysis for long-term investing.\n",
    "\n",
    "#### 2. Regime Detection:\n",
    "\n",
    "\"Apply a clustering algorithm of your choice (e.g. k-means, Gaussian Mixture Models, hierarchical clustering) To identify latent regimes (ie clusters) from this multivariate time series.\"\n",
    "\n",
    "For this assignment, I will be attempting the clustering algorithm; K-Means.\n",
    "\n",
    "#### 3. Integration with Prediction\n",
    "\"Use the detected regime labels as additional features in your predictive modelling task from Part 1 (the Student class with fit/predict).\n",
    "â€¢ Evaluate whether regime-informed prediction improves over your Part 1 Student class as the baseline\"\n",
    "\n",
    "For this assignment, I will attempt to integrate my chosen clustering algorithm with my original part 1 baseline student.py. I will also run experiments to see if certain fields/methods, e.t.c. changed within my *Regime Detection* section can help improve the overall Directional Accuracy and decrease the overall MAE and RMSE.\n",
    "\n",
    "#### 4. Baseline Comparison\n",
    "\n",
    "\"In addition to your own Part 1 baseline, you must compare your results against at least one published baseline method\" I will be using: \"McGreevy, J., Muguruza, A., Issa, Z., Salvi, C., Chan, J., & Zuric, Z. (2024). Detecting Multivariate Market Regimes Via Clustering Algorithms. SSRN [http://dx.doi.org/10.2139/ssrn.4758243]\"[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e390fb43",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "The clustering algorithm I'm attempting is K-Means Clustering. K-means has been proven as a successful clustering algorithm in previous academic papers, such as \"Horvath, Issa & Muguruza (2021); Clustering Market Regimes Using the Wasserstein Distance\"[2]; found that using Wasserstein (a variant of k-means, more on this later) got high accuracy in detecting market regimes in univariate time series.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720b599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "K = 2\n",
      "Silhouette: 0.334 | CH: 140.4\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0072, vol=0.0113 -> BULL\n",
      "Cluster 1: ret=-0.0193, vol=0.0140 -> BEAR\n",
      "Saved: detected_regimes_k2.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 3\n",
      "Silhouette: 0.303 | CH: 141.0\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0204, vol=0.0137 -> BULL\n",
      "Cluster 1: ret=0.0035, vol=0.0109 -> NEUTRAL\n",
      "Cluster 2: ret=-0.0300, vol=0.0141 -> BEAR\n",
      "Saved: detected_regimes_k3.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 4\n",
      "Silhouette: 0.149 | CH: 120.1\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=-0.0065, vol=0.0111 -> NEUTRAL\n",
      "Cluster 1: ret=0.0192, vol=0.0142 -> NEUTRAL\n",
      "Cluster 2: ret=-0.0353, vol=0.0157 -> BEAR\n",
      "Cluster 3: ret=0.0116, vol=0.0106 -> BULL\n",
      "Saved: detected_regimes_k4.csv\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run DataPreparation/KMeans_Data_Prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab6f68",
   "metadata": {},
   "source": [
    "Within Appendix/DataPreparation/KMeans_Data_Prep.py; I loaded daily prices from prices.csv (specifically adj_close), normalised tickers and converted long format into a wide panel of the six sector ETFs. From this weekly panel, I created causal features including weekly log returns, 12-week rolling volatility and 6-week momentum for each ticker, along with market level aggregates (i.e. cross sectional volaitility) for capturing overall market conditions. I drop rows with incomplete features and then split the dataset into train and test sets based on a fixed date (I chose 4th Jan 2019 as it is a Friday, it provides 9 years of training data (2010-2018) and 5 years of testing data (2019-2023)) as to avoid look-ahead bias. \n",
    "\n",
    "In an Introduction to Statistical Learning (ISLP)[3] it suggests \"... always running K-means clustering with a large value of n_init, such as 20 or 50, since otherwise an undesirable local optimum may be obtained\", hence why my n_init=50. Later in this assignment, I will trying n_init values of 20 and 30.\n",
    "\n",
    "As a practical consideration, for standardising inputs, I use StandardScaler to minimise the within-cluster Euclidean distances. As mentioned on page 532 of ISLP [3], standardisation is preferred on K-means, so later in this assignment, I will be trying a different scaler from the sklearn.preprocessing package to see which returns the best directional accuracy and lowest MAE and RMSE.\n",
    "\n",
    "To understand the best K value for my k-means clustering on the prices.csv data, I've opted to use a mixture of silhouette score and Calinski-Harabasz scores. To combat data leakage, I've included the creation of the regime_lag1 so that predictiosn only use information available at prediction time. \n",
    "\n",
    "Interpreting the Data;\n",
    "The terms; \"Bull\", \"Neutral\" and \"Bear\" are used to refer to stock market conditions - used to describe how the market is doing in general. According to Investopedia [4], \"A bull market is a market that is on the rise and where the economy is sound. A bear market exists in an economy that is receding, where most stocks are declining in value.\" Neutral market indicates a market that is stable, which is neither rising, nor falling.\n",
    "\n",
    "Within my data preparation file, I have a heuristic rule for the different K values.\n",
    "\n",
    "For K=2, I use binary classification on returns, where if average return is posivtive it is a \"Bull\", whereas negative it is \"Bear\".\n",
    "\n",
    "For K=3 and K=4, I use ternary classification on return and volatility, to create a 2x2 decision matrix:\n",
    "\n",
    "\n",
    "| Return VS Median | Volatility VS Median | Label   |\n",
    "|-------------------|-----------------------|---------|\n",
    "| Above Median      | Below Median         | Bull    |\n",
    "| Above Median      | Above Median         | Neutral |\n",
    "| Below Median      | Below Median         | Neutral |\n",
    "| Below Median      | Above Median         | Bear    |\n",
    "\n",
    "\n",
    "I use siholuette and Calinski-Harabasz to understand what would be the best K value. \n",
    "\n",
    "Use of Silhouette Score:\n",
    "The silhouette score ranges from -1 to 1; \"with the score being bounded between -1 and 1. [A positive value] indicates that the point is appropriately clustered, whereas values near 0 denote ambiguity, and negative values hint at a possible misclassification.\" according to NumberAnalytics[5].\n",
    "\n",
    "Use of Calinski-Harabasz (CH):\n",
    "The Calinski-Harabasz index (also known as variance ratio criterion) measures \"how similar an object is to it's own cluster (cohesion) compared to other clusters (seperation)\" according to GeeksForGeeks[6]. A higher value for CH means that clusters are dense are well seperated. \n",
    "\n",
    "Results:\n",
    "For K=2:\n",
    "- Silhouette Score is 0.334, CH Index is 140.4\n",
    "- Cluster 0 is bull\n",
    "- Cluster 1 is bear\n",
    "\n",
    "The silhouette score is the highest out of the different K values, indicating the clearest regime distinction. This is the most optimal k value as it's the simplest model for capturing the main market states.\n",
    "\n",
    "For K=3:\n",
    "-  Silhouette Score is 0.303, CH index is 141.0\n",
    "- Cluster 0 is bull\n",
    "- Cluster 1 is neutral\n",
    "- Cluster 2 is bear\n",
    "\n",
    "The silhouette score is lower than it is for k=2, but CH index is slightly higher. Silhouette score is more important for intrepability however, as Silhouette has a bounded range (-1 to 1) and so is more interpretable, compared to CH, which has no bounded range. This k value captures netural periods (slightly positive returns, low volatility).\n",
    "\n",
    "For K=4\n",
    "- Silhouette Score is 0.149, CH index is 120.1\n",
    "\n",
    "The Silhouette score drops dramatically to 0.149 and CH index also has the lowest value out of all the k values; indicating the worst seperation.\n",
    "\n",
    "For this assignment, I will be using k=2, but will investigate further with k=3 and k=4 in my experimental section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2883c",
   "metadata": {},
   "source": [
    "### Integrating with Prediction:\n",
    "\n",
    "Below is my baseline student results from Coursework Part 1, saved as \"student.py\" which uses ElasticNet (available to view in /Appendix as \"student.py\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba48f067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HORIZON  DIRACC    MAE   RMSE\n",
      "0        1   0.525  0.009  0.013\n",
      "1        5   0.560  0.019  0.027\n",
      "2       10   0.578  0.027  0.038\n",
      "3       20   0.598  0.038  0.053\n",
      "4       40   0.636  0.051  0.070\n",
      "5       50   0.656  0.056  0.076\n",
      "6       70   0.682  0.064  0.086\n",
      "7      100   0.706  0.074  0.099\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "filename = \"Results/part1_results_student.csv\"  \n",
    "df = pd.read_csv(filename)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670be434",
   "metadata": {},
   "source": [
    "\n",
    "To integrate my initial student.py from Coursework part 1 with Kmeans; I extended the class (please see Appendix/student_kmeans_before_experimentation.py) by creating the external file (via Appendix/KMeans_Data_Prep.py file, \"detected_regimes_k{K}.csv\", e.g. detected_regimes_k2.csv) and making it required in my prediction class: student_kmeans_before_experimentation.py\n",
    "\n",
    "Within my prediction class, I've made it so that the columns; date, regime and regime_lag1 are required (for no data leakage) input is the X dataframe (same as the original code) and also my detected_regimes_k2.csv. DateTimeIndex is sorted because time series operations like .loc[date] and shift() assume chronological order.\n",
    "\n",
    "For the _make_features() function in my prediction class,  it now has 13 features instead of 12 (original part 1 student features with my newly created regime column). Helper methods are same as my original part 1 student code. \n",
    "\n",
    "My prediction class has a new function; _get_regime_for_date(); this is a time-safe regime lookup function using the lagged regime data, again to ensure no data leakage (by ensuring regime at time t uses information up to t-1) and includes some handling of edge cases, such as NaN values.\n",
    "\n",
    "For the fit() function, I've added one-hot encoding of regime labels (lines 200-203), creating regime-specific indicator features. Additionally, I've stored the training column strucutre to ensure consistent feature alignment during prediction.  \n",
    "\n",
    "For predict() I have added column alignment logic to ensure prediction features match the training structure, including creating one-hot regime columns for new data and reordering columns to maintain consistency with training phase.\n",
    "\n",
    "Compared to my original student, my prediction class contains error-handling, such as validating that  the regime file has required columns, handles regimes not seen during training, aligns prediction features to training structures and manages int64 nullable types and is overall more robust than my original student.py class. \n",
    "\n",
    "Within my kmeans student class, I've also embedded the core functions of mltester; specifically forward_log_return(), compute_metrics(), walk_forward_predict(), evaluate_ticker(). Within my walk_forward_predict() (compared to the original mltester.py) I made it so that each walk-forward block gets a fresh model with the same regime file. I've also simplified data loading, compared to the original mltester.py.\n",
    "\n",
    "Looking at the results of the given DirAcc, MAE and RMSE from different K values, it successfully demonstrates that regime detection works due to differenet results, and as expected, K=2 performs best as it has better DirAcc and lower MAE/RMSE on average compared to K=3 and K=4. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ea28e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HORIZON  DIRACC     MAE    RMSE\n",
      "0        1   0.529  0.0086  0.0127\n",
      "1        5   0.569  0.0192  0.0270\n",
      "2       10   0.589  0.0267  0.0376\n",
      "3       20   0.619  0.0374  0.0525\n",
      "4       40   0.657  0.0510  0.0696\n",
      "5       50   0.677  0.0564  0.0764\n",
      "6       70   0.713  0.0639  0.0858\n",
      "7      100   0.737  0.0735  0.0983\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#these results were generated from the .csv outputted from Appendix/DataPreparation/KMeans_Data_Prep.py, \n",
    "# and the results were produced from \"Appendix/student_kmeans_before_experimentation.py\" - the results are saved in the below .csv file:\n",
    "\n",
    "filename = \"Results/student_kmeans_k2_standardscaler_init50_NOpca_results.csv\"  \n",
    "df = pd.read_csv(filename)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7e334",
   "metadata": {},
   "source": [
    "### Experimentation of Different Methods within my KMeans clustering.\n",
    "\n",
    "As of right now, my Kmeans clustering file (Appendix/DataPreparation/KMeans_Data_Prep.py) uses;\n",
    "- StandardScaler(),\n",
    "- No PCA used to generate clusters,\n",
    "- n_init = 50,\n",
    "\n",
    "In an attempt to improve the Directional Accuracy and decrease the MAE and RMSE values; I will try:\n",
    "- changing the StandardScaler() to RobustScaler(),\n",
    "- implement PCA within the cluster generation,\n",
    "- changing the n_init value to 20 and \n",
    "- changing the n_init value to 30 \n",
    "\n",
    "\n",
    "Below I've detailed my experimentation journey for each experiment attempted. \n",
    "\n",
    "#### RobustScaler (aka Robust):\n",
    "\n",
    "Within my KMeans_Data_Prep, I changed the scaler used on line 42 from StandardScaler() to RobustScaler() (Appendix/Experimentation/KMeans_Data_Prep_robustScaler). And changed the scaler used in the prediction file (Appendix/Experimentation/student_kmeans_multvariate_robust.py) which runs the mltester functionality on my computed timeseries. I decided to try replacing StandardScaler() with RobustScaler() as StandardScaler() (according to GeeksForGeeks [7]), \"subtracts the mean of the data and divides it by standard deviation. This centers the data around zero and standardizes the variablity\". It is more sensitive to outliers. Whereas RobustScaler \"subtracts the median of data and divides by interquartile range (IQR) which helps in reducing the effect of outliers why maintaining distribution of non-outlier values.\" I expect that RobustScaler will produce better results as it will make the prediction class less sensitve to extreme events.\n",
    "\n",
    "#### PCA:\n",
    "Within my KMeans_Data_Prep, I've added PCA (principle component analysis) for dimensionality reduction and to help \"reduce the number of features in a dataset while keeping the most important information\"  according to GeeksForGeeks[8]. This in itself can lead to less noise and redundant information, resulting in cleaner, more stable clusters. PCA also looks to mitigate the curse of dimensionality , as in high-dimensional spaces data can become sparse and can lead to overfitting. I expect the inclusion of PCA to produce better results due to noise reduction.\n",
    "\n",
    "#### n_init20 (aka Init20) and n_init30 (aka Init30)\n",
    "Within my KMeans_Data_Prep, I've changed the n_init value from 50 to 20 and 30 by changing the n_init parameter within the kmeans initialisation (line 55).\n",
    "The n_init parameter is defined as the \"Number of times the k-means algorithm is run with different centroid seeds\" according to the scikit-learn docs[9]. For this parameter (for both n_init=20 and n_init=30), I am expecting similar results to my current prediction class results: \"\"Results/student_kmeans_standardscaler_init50_NOpca_results.csv\" (printed above this cell).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1425f1f",
   "metadata": {},
   "source": [
    "#### Directional Accuracy Results\n",
    "\n",
    "![directional_accuracy_zoomed.png](Appendix/Images/directional_accuracy_zoomed1.png)\n",
    "(The above graph is zoomed in for easier distinction between bars)\n",
    "\n",
    "\n",
    "\n",
    "From the results; \n",
    "- Both the Init20 and Init30 have the same averages as each other across the differing k values, indicating that changing the n_init value does not make a difference to the directional accuracy.\n",
    "- For PCA addtion in regime detection; it has a high DirAcc in k=2 and k=3, then second highest in k=4, but stil higher than my original kmeans code. \n",
    "- For RobustScaler; it is the lowest scoring method for DirAcc in k=2, the same as other  for k=3 but has the highest score for k=4.\n",
    "- For my original student kmeans, it has a high DirAcc in k=2 and k=3, but then has the lowest DirAcc overall in K=4, indicating that my baseline clustering has become unstable at higher K values.\n",
    "\n",
    "Worst performing was Robust in K=2, then for k=4 it was init20, init 30 and my kmeans student.\n",
    "\n",
    "Overall I have found that regimes can improve predictional accuracy. For K=2, the highest directional accuracy is given by init20, init30, PCA and my original kmeans student. DirAcc is the same across all methods at k=3 and then Robust has the highest DirAcc at K=4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbdb5d",
   "metadata": {},
   "source": [
    "#### MAE Results\n",
    "\n",
    "![mae_zoomed.png](Appendix/Images/mae_zoomed1.png)\n",
    "(The above graph is zoomed in for easier distinction between bars)\n",
    "\n",
    "From the results:\n",
    "- Similar to DirAcc, both the Init20 and Init30 have the same averages for MAE as each other for the differing K values, indicating that changing the n_init value does not make a difference to MAE score.\n",
    "- PCA has the third highest MAE score for k=2, then the highest MAE for k=3, then second lowest for k=4.\n",
    "- Robust has the lowest MAE score overall at all k values.\n",
    "- My original kmeans student has the highest MAE at k=2, second highest at k=3 and has the same MAE value as init20 and init30 at k=4.\n",
    "\n",
    "Overall, for minimising the MAE score, RobustScaler() performed best in all k values. Worst performing (as in, highest scoring) was my student kmeans in k=2, PCA for k=3, then init20, init30 and my student kmeans for k=4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4afcff",
   "metadata": {},
   "source": [
    "#### RMSE Results\n",
    "\n",
    "![rmse_zoomed.png](Appendix/Images/rmse_zoomed1.png)\n",
    "(The above graph is zoomed in for easier distinction between bars)\n",
    "\n",
    "Results are very similar to results in MAE in regards to output. Overall, for minimising the RMSE score, RobustScaler() performed best in all k values. Worst performing (as in, highest scoring) was my student kmeans in k=2, PCA for k=3, then init20, init30 and my student kmeans for k=4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b430c8c",
   "metadata": {},
   "source": [
    "#### Decisions made based on results\n",
    "\n",
    "Based upon the results of my experimentation, I must consider the trade-off of having higher directional accuracy with higher MAE and RMSE, or slightly lower directional accuracy with lower MAE and RMSE. As RobustScaler() has shown to be stable throughout DirAcc, MAE and RMSE results, (especially considering the difference between the highest scoring methods and Robust for k=2 was 0.0008), I have decided to implement it within my prediction model, now to become student_ext.py.\n",
    "\n",
    "To run my student_ext.py, I will first prepare the data (now with using RobustScaler file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c4b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "K = 2\n",
      "Silhouette: 0.334 | CH: 144.5\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0075, vol=0.0114 -> BULL\n",
      "Cluster 1: ret=-0.0207, vol=0.0137 -> BEAR\n",
      "Saved: detected_regimes_k2_robustscaler.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 3\n",
      "Silhouette: 0.296 | CH: 142.5\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=0.0211, vol=0.0137 -> BULL\n",
      "Cluster 1: ret=-0.0294, vol=0.0140 -> BEAR\n",
      "Cluster 2: ret=0.0035, vol=0.0109 -> NEUTRAL\n",
      "Saved: detected_regimes_k3_robustscaler.csv\n",
      "\n",
      "-------------------------------\n",
      "K = 4\n",
      "Silhouette: 0.154 | CH: 122.5\n",
      "\n",
      "[INTERPRETATION - TRAIN]\n",
      "Cluster 0: ret=-0.0067, vol=0.0111 -> NEUTRAL\n",
      "Cluster 1: ret=0.0195, vol=0.0141 -> NEUTRAL\n",
      "Cluster 2: ret=0.0115, vol=0.0106 -> BULL\n",
      "Cluster 3: ret=-0.0361, vol=0.0158 -> BEAR\n",
      "Saved: detected_regimes_k4_robustscaler.csv\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%run DataPreparation/KMeans_Data_Prep_robustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0514a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING K=2 REGIMES\n",
      "==================================================\n",
      "\n",
      "Evaluating horizon 1 days...\n",
      "   XLE: DirAcc=0.5127  MAE=0.011796  RMSE=0.017381\n",
      "   XLF: DirAcc=0.5148  MAE=0.009424  RMSE=0.013988\n",
      "   XLI: DirAcc=0.5376  MAE=0.008376  RMSE=0.012288\n",
      "   XLK: DirAcc=0.5490  MAE=0.009201  RMSE=0.013440\n",
      "   XLP: DirAcc=0.5323  MAE=0.005938  RMSE=0.008649\n",
      "   XLV: DirAcc=0.5281  MAE=0.007118  RMSE=0.010159\n",
      "Horizon   1 days: DirAcc=0.529 | MAE=0.0086 | RMSE=0.0127\n",
      "\n",
      "Evaluating horizon 5 days...\n",
      "   XLE: DirAcc=0.5305  MAE=0.026839  RMSE=0.039030\n",
      "   XLF: DirAcc=0.5568  MAE=0.020812  RMSE=0.029535\n",
      "   XLI: DirAcc=0.5807  MAE=0.019007  RMSE=0.026991\n",
      "   XLK: DirAcc=0.5958  MAE=0.020012  RMSE=0.027484\n",
      "   XLP: DirAcc=0.5791  MAE=0.012665  RMSE=0.017620\n",
      "   XLV: DirAcc=0.5714  MAE=0.015699  RMSE=0.021462\n",
      "Horizon   5 days: DirAcc=0.569 | MAE=0.0192 | RMSE=0.0270\n",
      "\n",
      "Evaluating horizon 10 days...\n",
      "   XLE: DirAcc=0.5349  MAE=0.038098  RMSE=0.055896\n",
      "   XLF: DirAcc=0.5857  MAE=0.028969  RMSE=0.041096\n",
      "   XLI: DirAcc=0.5937  MAE=0.026170  RMSE=0.037358\n",
      "   XLK: DirAcc=0.6256  MAE=0.027848  RMSE=0.037391\n",
      "   XLP: DirAcc=0.6096  MAE=0.017613  RMSE=0.024215\n",
      "   XLV: DirAcc=0.5969  MAE=0.021666  RMSE=0.029307\n",
      "Horizon  10 days: DirAcc=0.591 | MAE=0.0267 | RMSE=0.0375\n",
      "\n",
      "Evaluating horizon 20 days...\n",
      "   XLE: DirAcc=0.5513  MAE=0.054686  RMSE=0.080323\n",
      "   XLF: DirAcc=0.6136  MAE=0.040658  RMSE=0.058044\n",
      "   XLI: DirAcc=0.6123  MAE=0.036587  RMSE=0.052929\n",
      "   XLK: DirAcc=0.6683  MAE=0.038226  RMSE=0.051351\n",
      "   XLP: DirAcc=0.6342  MAE=0.024745  RMSE=0.033222\n",
      "   XLV: DirAcc=0.6296  MAE=0.029413  RMSE=0.039270\n",
      "Horizon  20 days: DirAcc=0.618 | MAE=0.0374 | RMSE=0.0525\n",
      "\n",
      "Evaluating horizon 40 days...\n",
      "   XLE: DirAcc=0.5628  MAE=0.075476  RMSE=0.107326\n",
      "   XLF: DirAcc=0.6437  MAE=0.056200  RMSE=0.078159\n",
      "   XLI: DirAcc=0.6713  MAE=0.049333  RMSE=0.069561\n",
      "   XLK: DirAcc=0.7064  MAE=0.050482  RMSE=0.066895\n",
      "   XLP: DirAcc=0.6780  MAE=0.033325  RMSE=0.043009\n",
      "   XLV: DirAcc=0.6705  MAE=0.039773  RMSE=0.051344\n",
      "Horizon  40 days: DirAcc=0.655 | MAE=0.0508 | RMSE=0.0694\n",
      "\n",
      "Evaluating horizon 50 days...\n",
      "   XLE: DirAcc=0.5719  MAE=0.085017  RMSE=0.119038\n",
      "   XLF: DirAcc=0.6755  MAE=0.062722  RMSE=0.087044\n",
      "   XLI: DirAcc=0.6938  MAE=0.054674  RMSE=0.076470\n",
      "   XLK: DirAcc=0.7354  MAE=0.054189  RMSE=0.071614\n",
      "   XLP: DirAcc=0.6925  MAE=0.036502  RMSE=0.046255\n",
      "   XLV: DirAcc=0.6949  MAE=0.043104  RMSE=0.055568\n",
      "Horizon  50 days: DirAcc=0.677 | MAE=0.0560 | RMSE=0.0760\n",
      "\n",
      "Evaluating horizon 70 days...\n",
      "   XLE: DirAcc=0.6006  MAE=0.097168  RMSE=0.135109\n",
      "   XLF: DirAcc=0.6816  MAE=0.074465  RMSE=0.100052\n",
      "   XLI: DirAcc=0.7110  MAE=0.063786  RMSE=0.086968\n",
      "   XLK: DirAcc=0.7602  MAE=0.059973  RMSE=0.079322\n",
      "   XLP: DirAcc=0.7443  MAE=0.038907  RMSE=0.049735\n",
      "   XLV: DirAcc=0.7448  MAE=0.047223  RMSE=0.060286\n",
      "Horizon  70 days: DirAcc=0.707 | MAE=0.0636 | RMSE=0.0852\n",
      "\n",
      "Evaluating horizon 100 days...\n",
      "   XLE: DirAcc=0.6256  MAE=0.113105  RMSE=0.158020\n",
      "   XLF: DirAcc=0.7092  MAE=0.088826  RMSE=0.116786\n",
      "   XLI: DirAcc=0.7356  MAE=0.072352  RMSE=0.097667\n",
      "   XLK: DirAcc=0.8121  MAE=0.068349  RMSE=0.093145\n",
      "   XLP: DirAcc=0.7901  MAE=0.043061  RMSE=0.055151\n",
      "   XLV: DirAcc=0.7522  MAE=0.053833  RMSE=0.067126\n",
      "Horizon 100 days: DirAcc=0.737 | MAE=0.0733 | RMSE=0.0980\n",
      "\n",
      "K=2 Summary:\n",
      "Average DirAcc across horizons: 0.636\n",
      "Average MAE across horizons:    0.0419\n",
      "Average RMSE across horizons:   0.0573\n"
     ]
    }
   ],
   "source": [
    "%run student_ext.py #estimated run time: 63 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dbdee4",
   "metadata": {},
   "source": [
    "The following visualisation compare 3 models; the original part 1 baseline (no regime detection), my KMeans model with StandardScaler() (before experimentation) and my new KMeans class (after experimentation- adding RobustScaler() instead of StandardScaler()).\n",
    "\n",
    "![part1baseline_originalkmeans_robustkmeans.png](Appendix/Images/part1baseline_originalkmeans_robustkmeans.png)\n",
    "\n",
    "I've also included a heatmap to view results:\n",
    "\n",
    "![comparison_heatmaps.png](Appendix/Images/comparison_heatmaps.png)\n",
    "\n",
    "\n",
    "Overall, I have found that both KMeans classes have outperformed the Part1 baseline across all horizons; with smaller improvements in shortterm horizons and larger improvements in longterm horizons in regards to directional accuracy and MAE and RMSE has been reduced (a small amount) in some places. This demonstrates the value of incorporating market regime information.\n",
    "\n",
    "RobustScaler() vs StandardScaler() Trade Offs:\n",
    "\n",
    "My original kmeans before experimentation, using StandardScaler(), has slightly better directional accuracy (for example, at horizon 70), but my kmeans after experimentation, using RobustScaler() has lower RMSE than the original kmeans. As said before, due to RobustScaler() having good stability ( and with differences in directional accuracy being negligible), this version has become my student_ext.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14fa0d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: regimes_mcgreevy_weekly.csv\n"
     ]
    }
   ],
   "source": [
    "%run Appendix/PublishedComparison/ComputeRegimes_McGreevy.py\n",
    "\n",
    "# this creates regimes_mcgreevy_weekly.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc44a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TESTING K=2 REGIMES\n",
      "==================================================\n",
      "\n",
      "Evaluating horizon 1 days...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:627\u001b[39m, in \u001b[36mpandas._libs.index.DatetimeEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[39m, in \u001b[36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 1381363200000000000",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:595\u001b[39m, in \u001b[36mpandas._libs.index.DatetimeEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:629\u001b[39m, in \u001b[36mpandas._libs.index.DatetimeEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: Timestamp('2013-10-10 00:00:00')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:630\u001b[39m, in \u001b[36mDatetimeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: Timestamp('2013-10-10 00:00:00')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6853\u001b[39m, in \u001b[36mIndex.get_slice_bound\u001b[39m\u001b[34m(self, label, side)\u001b[39m\n\u001b[32m   6852\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6853\u001b[39m     slc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6854\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:632\u001b[39m, in \u001b[36mDatetimeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m632\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(orig_key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: Timestamp('2013-10-10 00:00:00')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\Appendix\\PublishedComparison\\student_ext_mcgreevy.py:458\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m#TODO add graphs if time allows - I need to focus on experimenting\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\Appendix\\PublishedComparison\\student_ext_mcgreevy.py:416\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    413\u001b[39m student = Student(regime_file=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mregimes_mcgreevy_weekly.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Evaluate ticker\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m diracc, mae, rmse = \u001b[43mstudent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate_ticker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mticker_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m rows.append({\n\u001b[32m    423\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m'\u001b[39m: t,\n\u001b[32m    424\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mdiracc\u001b[39m\u001b[33m'\u001b[39m: diracc,\n\u001b[32m    425\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m'\u001b[39m: mae,\n\u001b[32m    426\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrmse\u001b[39m\u001b[33m'\u001b[39m: rmse\n\u001b[32m    427\u001b[39m })\n\u001b[32m    429\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: DirAcc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdiracc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\Appendix\\PublishedComparison\\student_ext_mcgreevy.py:168\u001b[39m, in \u001b[36mStudent.evaluate_ticker\u001b[39m\u001b[34m(self, prices, horizon, step)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_ticker\u001b[39m(\u001b[38;5;28mself\u001b[39m, prices: pd.DataFrame, horizon: \u001b[38;5;28mint\u001b[39m = \u001b[32m1\u001b[39m, step: \u001b[38;5;28mint\u001b[39m = \u001b[32m10\u001b[39m):\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m#evaluating single ticker and return metrics \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     y_true, y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwalk_forward_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     diracc, mae, rmse = \u001b[38;5;28mself\u001b[39m.compute_metrics(y_true, y_pred)\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m diracc, mae, rmse\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\Appendix\\PublishedComparison\\student_ext_mcgreevy.py:154\u001b[39m, in \u001b[36mStudent.walk_forward_predict\u001b[39m\u001b[34m(self, prices, horizon, step)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# Create fresh model for this block\u001b[39;00m\n\u001b[32m    140\u001b[39m model = Student(\n\u001b[32m    141\u001b[39m     n_lags=\u001b[38;5;28mself\u001b[39m.n_lags,\n\u001b[32m    142\u001b[39m     mom_windows=\u001b[38;5;28mself\u001b[39m.mom_windows,\n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m     regime_file=\u001b[38;5;28mself\u001b[39m.regime_file\n\u001b[32m    152\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhorizon\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m X_pred = prices.loc[: block[-\u001b[32m1\u001b[39m]]\n\u001b[32m    156\u001b[39m y_hat = model.predict(X_pred, meta={\u001b[33m\"\u001b[39m\u001b[33mhorizon\u001b[39m\u001b[33m\"\u001b[39m: horizon})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\Appendix\\PublishedComparison\\student_ext_mcgreevy.py:265\u001b[39m, in \u001b[36mStudent.fit\u001b[39m\u001b[34m(self, X_train, y_train, meta)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train: pd.DataFrame, y_train: pd.Series, meta=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     F = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m F.empty:\n\u001b[32m    268\u001b[39m         mean_y = \u001b[38;5;28mself\u001b[39m._finite_mean(y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\Appendix\\PublishedComparison\\student_ext_mcgreevy.py:258\u001b[39m, in \u001b[36mStudent._make_features\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m#adding regime featuresa\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m regime_labels = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_regime_for_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m F[\u001b[33m'\u001b[39m\u001b[33mregime\u001b[39m\u001b[33m'\u001b[39m] = regime_labels  \n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6537\u001b[39m, in \u001b[36mIndex.map\u001b[39m\u001b[34m(self, mapper, na_action)\u001b[39m\n\u001b[32m   6501\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6502\u001b[39m \u001b[33;03mMap values using an input mapping or function.\u001b[39;00m\n\u001b[32m   6503\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6533\u001b[39m \u001b[33;03mIndex(['A', 'B', 'C'], dtype='object')\u001b[39;00m\n\u001b[32m   6534\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6535\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiIndex\n\u001b[32m-> \u001b[39m\u001b[32m6537\u001b[39m new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6539\u001b[39m \u001b[38;5;66;03m# we can return a MultiIndex\u001b[39;00m\n\u001b[32m   6540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_values.size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_values[\u001b[32m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\base.py:923\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    920\u001b[39m arr = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.map_array(arr, mapper, na_action=na_action, convert=convert)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:81\u001b[39m, in \u001b[36mravel_compat.<locals>.method\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(meth)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmethod\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     flags = \u001b[38;5;28mself\u001b[39m._ndarray.flags\n\u001b[32m     84\u001b[39m     flat = \u001b[38;5;28mself\u001b[39m.ravel(\u001b[33m\"\u001b[39m\u001b[33mK\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:763\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.map\u001b[39m\u001b[34m(self, mapper, na_action)\u001b[39m\n\u001b[32m    759\u001b[39m \u001b[38;5;129m@ravel_compat\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, mapper, na_action=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    761\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Index\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     result = \u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    764\u001b[39m     result = Index(result)\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, ABCMultiIndex):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - Queen's University Belfast\\Desktop\\MachineLearningAssignmentPt2\\ClusteringAttempts\\Appendix\\PublishedComparison\\student_ext_mcgreevy.py:209\u001b[39m, in \u001b[36mStudent._get_regime_for_date\u001b[39m\u001b[34m(self, date)\u001b[39m\n\u001b[32m    207\u001b[39m ts = pd.Timestamp(date)\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# using the lagged column to ensure never reading same-bar info\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregime_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mregime_lag1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mts\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m s.empty:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Default regime at the start of sample\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1192\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1190\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1191\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1412\u001b[39m, in \u001b[36m_LocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   1411\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_key(key, axis)\n\u001b[32m-> \u001b[39m\u001b[32m1412\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_slice_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m com.is_bool_indexer(key):\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getbool_axis(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexing.py:1444\u001b[39m, in \u001b[36m_LocIndexer._get_slice_axis\u001b[39m\u001b[34m(self, slice_obj, axis)\u001b[39m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj.copy(deep=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1443\u001b[39m labels = obj._get_axis(axis)\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m indexer = \u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   1447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._slice(indexer, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:682\u001b[39m, in \u001b[36mDatetimeIndex.slice_indexer\u001b[39m\u001b[34m(self, start, end, step)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;66;03m# GH#33146 if start and end are combinations of str and None and Index is not\u001b[39;00m\n\u001b[32m    675\u001b[39m \u001b[38;5;66;03m# monotonic, we can not use Index.slice_indexer because it does not honor the\u001b[39;00m\n\u001b[32m    676\u001b[39m \u001b[38;5;66;03m# actual elements, is only searching for start and end\u001b[39;00m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    678\u001b[39m     check_str_or_none(start)\n\u001b[32m    679\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m check_str_or_none(end)\n\u001b[32m    680\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_monotonic_increasing\n\u001b[32m    681\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    684\u001b[39m mask = np.array(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    685\u001b[39m in_index = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6708\u001b[39m, in \u001b[36mIndex.slice_indexer\u001b[39m\u001b[34m(self, start, end, step)\u001b[39m\n\u001b[32m   6664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mslice_indexer\u001b[39m(\n\u001b[32m   6665\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   6666\u001b[39m     start: Hashable | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   6667\u001b[39m     end: Hashable | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   6668\u001b[39m     step: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   6669\u001b[39m ) -> \u001b[38;5;28mslice\u001b[39m:\n\u001b[32m   6670\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6671\u001b[39m \u001b[33;03m    Compute the slice indexer for input labels and step.\u001b[39;00m\n\u001b[32m   6672\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6706\u001b[39m \u001b[33;03m    slice(1, 3, None)\u001b[39;00m\n\u001b[32m   6707\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6708\u001b[39m     start_slice, end_slice = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslice_locs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6710\u001b[39m     \u001b[38;5;66;03m# return a slice\u001b[39;00m\n\u001b[32m   6711\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(start_slice):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6940\u001b[39m, in \u001b[36mIndex.slice_locs\u001b[39m\u001b[34m(self, start, end, step)\u001b[39m\n\u001b[32m   6938\u001b[39m end_slice = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   6939\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6940\u001b[39m     end_slice = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_slice_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mright\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   6941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end_slice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6942\u001b[39m     end_slice = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6856\u001b[39m, in \u001b[36mIndex.get_slice_bound\u001b[39m\u001b[34m(self, label, side)\u001b[39m\n\u001b[32m   6854\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   6855\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m6856\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_searchsorted_monotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6857\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m   6858\u001b[39m         \u001b[38;5;66;03m# raise the original KeyError\u001b[39;00m\n\u001b[32m   6859\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6788\u001b[39m, in \u001b[36mIndex._searchsorted_monotonic\u001b[39m\u001b[34m(self, label, side)\u001b[39m\n\u001b[32m   6786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_searchsorted_monotonic\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, side: Literal[\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   6787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_monotonic_increasing:\n\u001b[32m-> \u001b[39m\u001b[32m6788\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m=\u001b[49m\u001b[43mside\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6789\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_monotonic_decreasing:\n\u001b[32m   6790\u001b[39m         \u001b[38;5;66;03m# np.searchsorted expects ascending sort order, have to reverse\u001b[39;00m\n\u001b[32m   6791\u001b[39m         \u001b[38;5;66;03m# everything for it to work (element ordering, search side and\u001b[39;00m\n\u001b[32m   6792\u001b[39m         \u001b[38;5;66;03m# resulting value).\u001b[39;00m\n\u001b[32m   6793\u001b[39m         pos = \u001b[38;5;28mself\u001b[39m[::-\u001b[32m1\u001b[39m].searchsorted(\n\u001b[32m   6794\u001b[39m             label, side=\u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m side == \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6795\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\base.py:1359\u001b[39m, in \u001b[36mIndexOpsMixin.searchsorted\u001b[39m\u001b[34m(self, value, side, sorter)\u001b[39m\n\u001b[32m   1356\u001b[39m values = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np.ndarray):\n\u001b[32m   1358\u001b[39m     \u001b[38;5;66;03m# Going through EA.searchsorted directly improves performance GH#38083\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m=\u001b[49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms.searchsorted(\n\u001b[32m   1362\u001b[39m     values,\n\u001b[32m   1363\u001b[39m     value,\n\u001b[32m   1364\u001b[39m     side=side,\n\u001b[32m   1365\u001b[39m     sorter=sorter,\n\u001b[32m   1366\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\40261885\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:248\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.searchsorted\u001b[39m\u001b[34m(self, value, side, sorter)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;129m@doc\u001b[39m(ExtensionArray.searchsorted)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearchsorted\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    245\u001b[39m     sorter: NumpySorter | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    246\u001b[39m ) -> npt.NDArray[np.intp] | np.intp:\n\u001b[32m    247\u001b[39m     npvalue = \u001b[38;5;28mself\u001b[39m._validate_setitem_value(value)\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ndarray\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearchsorted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mside\u001b[49m\u001b[43m=\u001b[49m\u001b[43mside\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorter\u001b[49m\u001b[43m=\u001b[49m\u001b[43msorter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%run Appendix/PublishedComparison/student_ext_mcgreevy.py #takes 59 mins to run - RESULTS OF THIS ARE IN \"Results/mcgreevy_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b12ad",
   "metadata": {},
   "source": [
    "## 4. Baseline Comparison\n",
    "\n",
    "For baseline comparison, I will be using the \"Detecting Multivariate Market Regimes via Clustering Algorithms\" method (2024) by James McGreevy (Aitor Muguruza-Gonzalez, Zacharia Issa Jonathan Chan and Cris Salvi). I prepared the data via ComputeRegimes_McGreevy.py (which outputs regimes_mcgreevy_weekly.csv) and ran it on student_ext_mcgreevy.py (only difference between this file and student_ext.py is that this file uses the the regimes_mcgreevy_weekly.csv outputted from ComputeRegimes_McGreevy.py).\n",
    "\n",
    "Key takeaways of the McGreevy et al paper is as follows:\n",
    "\n",
    "\"â€¢ We develop an adapted k-means algorithm that uses the 2-Wasserstein distance metric or Maximum Mean Discrepancy, and d-dimensional data in order to identify changes in joint market regimes between assets, in particular correlation.\n",
    "â€¢ We create a two-step process for finding the marginal and joint market regimes in synthetic and real data.\n",
    "â€¢ Using the two-step process, we form approximations to the mean, variance and correlation which then subsequently inform profitable portfolios of pairs of stocks.\"\n",
    "\n",
    "To begin creating my version of McGreevy et al for this assignment, I first created a script to compute regimes (ComputeRegimes_McGreevy.py), then also created the class containing the fit(), predict() and relevant mltester functions, that will use the computed regimes. For McGreevy et al (2024), the paper's main contribution is showing how it adapts clustering for the specific problem - using a probability metric (specifically; 2-Wasserstein distance between empirical measures) within it's clustering framework to compare the data segment distributions, rather than using raw data points. More specifically, the novelty of McGreevy et al's is replacing Euclidean distance with Wasserstein to compare data segment distributions.\n",
    "\n",
    "### Creation of the Regimes (ComputeRegimes_McGreevy.py)\n",
    "\n",
    "Following McGreevy et al's framework, I used log returns of the six sector ETFs. Weekly sampling (as I've mentioned before) reduces noise compared to daily data. By using the wide panel format (tickers as columns, dates as rows) it allows for direct application of their multivariate clustering approach. The paper states: \"We work with the log-returns of elements of the\n",
    "form S = (s0, . . . , sN ) âˆˆ S(R^d), which are price paths of d financial assets.\"\n",
    "\n",
    "Although McGreevy et al doesn't mention standardisation, to maintain consistency with my baseline models and ensure fair distance comparisons, I standardised returns within a walk-foward framework. \n",
    "\n",
    "![h1_minus_h2_block_mcgreevy.png](Appendix/Images/h1_minus_h2_block_mcgreevy.png)\n",
    "\n",
    "(Image is taken from another McGreevy et al paper (2023)).\n",
    "\n",
    "Figure 5.2 is a horizontal timeline of returns divided into boxes (where each box = an observation, e.g. one week). h1 represents the segment length (e.g. from the image; 5 boxes = 5 weeks). h2 represents the overlap between consecutive segments (e.g. spanning all 5 boxes / weeks). The red block indicates the current segment being clustered. The segments (\\mu_1, \\mu_2, e.t.c.) represents an empirical distribution of returns over h1 weeks, with the lower arrows showing an overlapping sliding forward windows by h1 - h2 steps. \n",
    "\n",
    "I implemented the paper's segmentation method using 20-week windows (h1) and 16 week overlaps (h2), resulting in a 4-week step between consecutive segments. As mentioned above, each segment represents an empirical distribution that forms the basis for regime detection.\n",
    "\n",
    "The core innovation in McGreevy et al's paper is replacing Euclidean distance with Wasserstein distance for comparing empirical distributions. Within my w2() function, I've implemented their 2-Wasserstein metric using optimal transport, specifically the Hungarian algorithm for segments (In \"The d-dimensional WK algorithm\": \"Computation of the p-Wasserstein distance... Hungarian algorithm\" (Hungarian being the \"linear_sum_assignment(C)\" in my code)).\n",
    "\n",
    "Following McGreevy's two step approach, for the first step, it focuses on marginal distributions (volatility patterns).  I first applied univariate Wasserstein k-means to each asset seperately, identifying high and low volatility regimes (k=2). McGreevy et al. (2024) explains that this step removes the influence of marginal distributions (mean and variance) before analysing correlation. They write: \"We begin by applying the uni-d 1-WK-means algorithm to the data in order to remove the effects of the marginal distribution on each asset and subsequently apply the 2-d WK-means or 2-d MMDK-means algorithm to this transformed data. This method is based on the theory of copulas.\"\n",
    "\n",
    "To follow McGreevy et al's recommendation of seperating volatility effects from correlation structure, I implemented a copula transformation. In my code, this happens after univariate clustering in ecdf_from_atoms() function. For each ETF and its volatility cluster, I built an empirical CDF (ECDF) from pooled segment values, then I converted each segment's returns into Uniform(0,1) values using this ECDF. This ensures that all marginals are standardised, leaving only dependence structure.\n",
    "\n",
    "When volatilty effects are removed via copula transfrmation, I performed multivariate Wasserstein k-means clustering on these copula segments to detect correlation regimes (k=2). The second stage identifies periods of high versus low interdependence among ETFs.\n",
    "\n",
    "Finally, Regime labels were lagged by one period to prevent look-ahead bias, as mentioned previously in my \"Data Preparation\" section. The outputted .csv file is used by \"student_ext_mcgreevy.py\" which uses regime_lag1 when merging features to avoid leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e6bcf",
   "metadata": {},
   "source": [
    "For the comparison itself I will be comparing my student_ext.py from this assignment with my McGreevy et al implemetation.\n",
    "\n",
    "For Directional Accuracy:\n",
    "\n",
    "![diracc_heatmap_large.png](Appendix/Images/diracc_heatmap_large.png)\n",
    "\n",
    "Results for DirAcc are quite similar with each other, but my student_ext.py outperforms at horizons 20 and 100, whereas McGreevy et al outperforms at horizons 40 and 70.\n",
    "\n",
    "For MAE:\n",
    "\n",
    "![mae_heatmap_large.png](Appendix/Images/mae_heatmap_large.png)\n",
    "\n",
    "Results for MAE are very similar, but student_ext.py outperforms at horizon 100, with a 0.01 smaller difference.\n",
    "\n",
    "\n",
    "For RMSE:\n",
    "\n",
    "![rmse_heatmap_large.png](Appendix/Images/rmse_heatmap_large.png)\n",
    "\n",
    "Again, Results for RMSE are very similar, but student_ext.py outperforms at horizon 10 and 20, with a 0.01 smaller difference for each.\n",
    "\n",
    "Overall, my McGreevy et al implementation was a strong competitor against my student_ext.py. My student_ext.py however still outperfomed (though with some very small differences), further validating the the effectiveness of my student_ext.py. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af785f1",
   "metadata": {},
   "source": [
    "References:\n",
    "[1] Detecting Multivariate Market Regimes via Clustering Algorithms, James Mc Greevy, Aitor Muguruza Gonzalez, Zacharia Issa, Jonathan Chan, Cris Salvi (2024) Imperial College London\n",
    "\n",
    "\n",
    "[2] Horvath, Issa & Muguruza (2021); Clustering Market Regimes Using the Wasserstein Distance (https://arxiv.org/abs/2110.11848 )\n",
    "\n",
    "\n",
    "[3] An Introduction to Statisitcal Learning; Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Johnathan Taylor (2023)\n",
    "\n",
    "\n",
    "[4] Investopedia https://www.investopedia.com/insights/digging-deeper-bull-and-bear-markets/ by Leslie Kramer, June 2024\n",
    "\n",
    "\n",
    "\n",
    "[5] NumberAnalytics https://www.numberanalytics.com/blog/silhouette-score-clustering-evaluation by Sarah Lee (AI GENERATED), March 2025\n",
    "\n",
    "[6] GeeksForGeeks https://www.geeksforgeeks.org/machine-learning/calinski-harabasz-index-cluster-validity-indices-set-3/ Calinski-Harabasz Index â€“ Cluster Validity indices, July 2025, Debomit Dey\n",
    "\n",
    "\n",
    "[7] GeeksForGeeks https://www.geeksforgeeks.org/machine-learning/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/  StandardScaler, MinMaxScaler and RobustScaler techniques, Oct 2025, Ashwin\n",
    "\n",
    "\n",
    "[8] GeeksForGeeks https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-pca/ Principal Component Analysis (PCA), Nov 2025, Aishwarya\n",
    "\n",
    "[9] https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html, 2025\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
